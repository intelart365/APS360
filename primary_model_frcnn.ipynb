{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "Vs6aTVg5xhcu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Compute Intersection over Union (IoU) between two sets of boxes.\n",
        "    :param box1: (Tensor of shape N x 4)\n",
        "    :param box2: (Tensor of shape M x 4)\n",
        "    :return: IoU matrix of shape N x M\n",
        "    \"\"\"\n",
        "    area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])  # N\n",
        "    area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])  # M\n",
        "\n",
        "    x1 = torch.max(box1[:, None, 0], box2[:, 0])  # N x M\n",
        "    y1 = torch.max(box1[:, None, 1], box2[:, 1])  # N x M\n",
        "    x2 = torch.min(box1[:, None, 2], box2[:, 2])  # N x M\n",
        "    y2 = torch.min(box1[:, None, 3], box2[:, 3])  # N x M\n",
        "\n",
        "    inter_area = (x2 - x1).clamp(min=0) * (y2 - y1).clamp(min=0)  # N x M\n",
        "    union_area = area1[:, None] + area2 - inter_area  # N x M\n",
        "    return inter_area / union_area  # N x M\n",
        "\n",
        "\n",
        "def boxes_to_transformation_targets(gt_boxes, anchor_boxes):\n",
        "    \"\"\"\n",
        "    Compute transformation targets (tx, ty, tw, th) for anchor boxes based on ground truth boxes.\n",
        "    :param gt_boxes: (N, 4) Ground truth bounding boxes (x1, y1, x2, y2)\n",
        "    :param anchor_boxes: (N, 4) Anchor boxes (x1, y1, x2, y2)\n",
        "    :return: (N, 4) Transformation targets (tx, ty, tw, th) for the anchor boxes\n",
        "    \"\"\"\n",
        "\n",
        "    # Get center_x, center_y, width, height for anchor boxes\n",
        "    anchor_widths = anchor_boxes[:, 2] - anchor_boxes[:, 0]\n",
        "    anchor_heights = anchor_boxes[:, 3] - anchor_boxes[:, 1]\n",
        "    anchor_center_x = anchor_boxes[:, 0] + 0.5 * anchor_widths\n",
        "    anchor_center_y = anchor_boxes[:, 1] + 0.5 * anchor_heights\n",
        "\n",
        "    # Get center_x, center_y, width, height for ground truth boxes\n",
        "    gt_widths = gt_boxes[:, 2] - gt_boxes[:, 0]\n",
        "    gt_heights = gt_boxes[:, 3] - gt_boxes[:, 1]\n",
        "    gt_center_x = gt_boxes[:, 0] + 0.5 * gt_widths\n",
        "    gt_center_y = gt_boxes[:, 1] + 0.5 * gt_heights\n",
        "\n",
        "    # Compute transformation targets (tx, ty, tw, th)\n",
        "    dx = (gt_center_x - anchor_center_x) / anchor_widths\n",
        "    dy = (gt_center_y - anchor_center_y) / anchor_heights\n",
        "    dw = torch.log(gt_widths / anchor_widths)\n",
        "    dh = torch.log(gt_heights / anchor_heights)\n",
        "\n",
        "    # Stack the targets\n",
        "    transformation_targets = torch.stack((dx, dy, dw, dh), dim=1)\n",
        "    return transformation_targets\n",
        "\n",
        "def apply_regression_pred_to_anchors(box_transform_pred, anchors):\n",
        "    \"\"\"\n",
        "    Apply predicted transformations to anchors to get predicted bounding boxes.\n",
        "    :param box_transform_pred: (num_anchors, num_classes, 4) Transformation predictions (dx, dy, dw, dh)\n",
        "    :param anchors: (num_anchors, 4) Anchor boxes (x1, y1, x2, y2)\n",
        "    :return: pred_boxes: (num_anchors, num_classes, 4) Predicted bounding boxes (x1, y1, x2, y2)\n",
        "    \"\"\"\n",
        "    box_transform_pred = box_transform_pred.reshape(\n",
        "        box_transform_pred.size(0), -1, 4)\n",
        "\n",
        "    # Get width, height, and center of anchors\n",
        "    w = anchors[:, 2] - anchors[:, 0]\n",
        "    h = anchors[:, 3] - anchors[:, 1]\n",
        "    center_x = anchors[:, 0] + 0.5 * w\n",
        "    center_y = anchors[:, 1] + 0.5 * h\n",
        "\n",
        "    dx = box_transform_pred[..., 0]\n",
        "    dy = box_transform_pred[..., 1]\n",
        "    dw = box_transform_pred[..., 2]\n",
        "    dh = box_transform_pred[..., 3]\n",
        "\n",
        "    # Clamp the dw, dh values to avoid large values\n",
        "    dw = torch.clamp(dw, max=math.log(1000.0 / 16))\n",
        "    dh = torch.clamp(dh, max=math.log(1000.0 / 16))\n",
        "\n",
        "    # Compute predicted center and dimensions\n",
        "    pred_center_x = dx * w[:, None] + center_x[:, None]\n",
        "    pred_center_y = dy * h[:, None] + center_y[:, None]\n",
        "    pred_w = torch.exp(dw) * w[:, None]\n",
        "    pred_h = torch.exp(dh) * h[:, None]\n",
        "\n",
        "    # Convert predicted center and dimensions to (x1, y1, x2, y2)\n",
        "    pred_box_x1 = pred_center_x - 0.5 * pred_w\n",
        "    pred_box_y1 = pred_center_y - 0.5 * pred_h\n",
        "    pred_box_x2 = pred_center_x + 0.5 * pred_w\n",
        "    pred_box_y2 = pred_center_y + 0.5 * pred_h\n",
        "\n",
        "    # Stack to get final predicted boxes\n",
        "    pred_boxes = torch.stack((pred_box_x1, pred_box_y1, pred_box_x2, pred_box_y2), dim=2)\n",
        "    return pred_boxes\n",
        "\n",
        "\n",
        "def sample_positive_negative(labels, positive_count, total_count):\n",
        "    # Sample positive and negative proposals for training\n",
        "    positive = torch.where(labels >= 1)[0]\n",
        "    negative = torch.where(labels == 0)[0]\n",
        "    num_pos = positive_count\n",
        "    num_pos = min(positive.numel(), num_pos)\n",
        "    num_neg = total_count - num_pos\n",
        "    num_neg = min(negative.numel(), num_neg)\n",
        "    perm_positive_idxs = torch.randperm(positive.numel(),\n",
        "                                        device=positive.device)[:num_pos]\n",
        "    perm_negative_idxs = torch.randperm(negative.numel(),\n",
        "                                        device=negative.device)[:num_neg]\n",
        "    pos_idxs = positive[perm_positive_idxs]\n",
        "    neg_idxs = negative[perm_negative_idxs]\n",
        "    sampled_pos_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
        "    sampled_neg_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
        "    sampled_pos_idx_mask[pos_idxs] = True\n",
        "    sampled_neg_idx_mask[neg_idxs] = True\n",
        "    return sampled_neg_idx_mask, sampled_pos_idx_mask\n",
        "\n",
        "\n",
        "def clamp_boxes_to_image_boundary(boxes, image_shape):\n",
        "    boxes_x1 = boxes[..., 0]\n",
        "    boxes_y1 = boxes[..., 1]\n",
        "    boxes_x2 = boxes[..., 2]\n",
        "    boxes_y2 = boxes[..., 3]\n",
        "    height, width = image_shape[-2:]\n",
        "    boxes_x1 = boxes_x1.clamp(min=0, max=width)\n",
        "    boxes_x2 = boxes_x2.clamp(min=0, max=width)\n",
        "    boxes_y1 = boxes_y1.clamp(min=0, max=height)\n",
        "    boxes_y2 = boxes_y2.clamp(min=0, max=height)\n",
        "    boxes = torch.cat((\n",
        "        boxes_x1[..., None],\n",
        "        boxes_y1[..., None],\n",
        "        boxes_x2[..., None],\n",
        "        boxes_y2[..., None]),\n",
        "        dim=-1)\n",
        "    return boxes\n",
        "\n",
        "class RegionProposalNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    RPN with following layers on the feature map\n",
        "        1. 3x3 conv layer followed by Relu\n",
        "        2. 1x1 classification conv with num_anchors(num_scales x num_aspect_ratios) output channels\n",
        "        3. 1x1 classification conv with 4 x num_anchors output channels\n",
        "\n",
        "    Classification is done via one value indicating probability of foreground\n",
        "    with sigmoid applied during inference\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, scales, aspect_ratios):\n",
        "        super(RegionProposalNetwork, self).__init__()\n",
        "        self.scales = scales\n",
        "        self.aspect_ratios = aspect_ratios\n",
        "        self.low_iou_threshold = 0.2\n",
        "        self.high_iou_threshold = 0.85\n",
        "        self.rpn_nms_threshold = 0.2\n",
        "        self.rpn_batch_size = 1\n",
        "        self.rpn_pos_count = int(1)\n",
        "        self.rpn_topk = 15 if self.training else 10\n",
        "        self.rpn_prenms_topk = 15 if self.training \\\n",
        "            else 10\n",
        "        self.num_anchors = len(self.scales) * len(self.aspect_ratios)\n",
        "\n",
        "        # 3x3 conv layer\n",
        "        self.rpn_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # 1x1 classification conv layer\n",
        "        self.cls_layer = nn.Conv2d(in_channels, self.num_anchors, kernel_size=1, stride=1)\n",
        "\n",
        "        # 1x1 regression\n",
        "        self.bbox_reg_layer = nn.Conv2d(in_channels, self.num_anchors * 4, kernel_size=1, stride=1)\n",
        "\n",
        "\n",
        "    def generate_anchors(self, image, feat):\n",
        "      \"\"\"\n",
        "      Generate anchor boxes for a feature map based on predefined scales and aspect ratios.\n",
        "      First, zero-centered anchors are created and then shifted across the feature map.\n",
        "      The anchor centers are at the top-left corners of each feature map cell.\n",
        "\n",
        "      :param image: (N, C, H, W) Tensor representing the image dimensions.\n",
        "      :param feat: (N, C_feat, H_feat, W_feat) Tensor representing the feature map dimensions.\n",
        "      :return: Anchors of shape (H_feat * W_feat * num_anchors_per_location, 4)\n",
        "      \"\"\"\n",
        "      grid_h, grid_w = feat.shape[-2:]\n",
        "      image_h, image_w = image.shape[-2:]\n",
        "\n",
        "      # Compute stride for each grid cell\n",
        "      stride_h = image_h // grid_h\n",
        "      stride_w = image_w // grid_w\n",
        "\n",
        "      # Convert scales and aspect ratios to tensors\n",
        "      scales = torch.as_tensor(self.scales, dtype=feat.dtype, device=feat.device)\n",
        "      aspect_ratios = torch.as_tensor(self.aspect_ratios, dtype=feat.dtype, device=feat.device)\n",
        "\n",
        "      # Calculate height and width ratios based on aspect ratios\n",
        "      h_ratios = torch.sqrt(aspect_ratios)\n",
        "      w_ratios = 1 / h_ratios\n",
        "\n",
        "      # Compute anchor widths and heights based on scales and aspect ratios\n",
        "      ws = (w_ratios[:, None] * scales[None, :]).view(-1)\n",
        "      hs = (h_ratios[:, None] * scales[None, :]).view(-1)\n",
        "\n",
        "      # Create zero-centered anchors (x1, y1, x2, y2)\n",
        "      base_anchors = torch.stack([-ws, -hs, ws, hs], dim=1) / 2\n",
        "      base_anchors = base_anchors.round()\n",
        "\n",
        "      # Compute shifts in x and y directions for the feature map\n",
        "      shifts_x = torch.arange(0, grid_w, dtype=torch.int32, device=feat.device) * stride_w\n",
        "      shifts_y = torch.arange(0, grid_h, dtype=torch.int32, device=feat.device) * stride_h\n",
        "\n",
        "      # Generate a grid of shifts\n",
        "      shifts_y, shifts_x = torch.meshgrid(shifts_y, shifts_x, indexing=\"ij\")\n",
        "\n",
        "      shifts_x = shifts_x.reshape(-1)\n",
        "      shifts_y = shifts_y.reshape(-1)\n",
        "\n",
        "      # Combine x and y shifts\n",
        "      shifts = torch.stack((shifts_x, shifts_y, shifts_x, shifts_y), dim=1)\n",
        "\n",
        "      # Add the shifts to the base anchors to create final anchors\n",
        "      anchors = (shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4))\n",
        "      anchors = anchors.reshape(-1, 4)\n",
        "\n",
        "      return anchors\n",
        "\n",
        "    def assign_targets_to_anchors(self, anchors, gt_boxes):\n",
        "        \"\"\"\n",
        "        Assign ground truth boxes to anchors based on IOU and generate classification labels.\n",
        "        - Label 1 for anchors with IOU above the high threshold (positive matches).\n",
        "        - Label 0 for anchors with IOU below the low threshold (background).\n",
        "        - Label -1 for anchors with IOU between the low and high thresholds (ignored).\n",
        "\n",
        "        :param anchors: (num_anchors, 4) Tensor containing anchor box coordinates.\n",
        "        :param gt_boxes: (num_gt_boxes, 4) Tensor containing ground truth box coordinates.\n",
        "        :return:\n",
        "            labels: (num_anchors) Tensor of labels {-1, 0, 1} for each anchor.\n",
        "            matched_gt_boxes: (num_anchors, 4) Tensor of assigned ground truth box coordinates for each anchor.\n",
        "        \"\"\"\n",
        "        # Compute the IOU matrix between anchors and ground truth boxes\n",
        "        iou_matrix = get_iou(gt_boxes, anchors)\n",
        "\n",
        "        # Find the ground truth box with the highest IOU for each anchor\n",
        "        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n",
        "\n",
        "        # Save the initial GT indices for later adjustments\n",
        "        best_match_gt_idx_pre_thresholding = best_match_gt_idx.clone()\n",
        "\n",
        "        # Apply thresholds to classify anchors:\n",
        "        below_low_threshold = best_match_iou < self.low_iou_threshold\n",
        "        between_thresholds = (best_match_iou >= self.low_iou_threshold) & (best_match_iou < self.high_iou_threshold)\n",
        "\n",
        "        # Mark anchors with low IOU as background (-1), and those between thresholds as ignored (-2)\n",
        "        best_match_gt_idx[below_low_threshold] = -1\n",
        "        best_match_gt_idx[between_thresholds] = -2\n",
        "\n",
        "        # Find anchors with the highest IOU for each ground truth box (to handle multiple matches)\n",
        "        best_anchor_iou_for_gt, _ = iou_matrix.max(dim=1)\n",
        "        gt_pred_pair_with_highest_iou = torch.where(iou_matrix == best_anchor_iou_for_gt[:, None])\n",
        "\n",
        "        # Get the indices of anchors that should be updated (positive matches)\n",
        "        pred_inds_to_update = gt_pred_pair_with_highest_iou[1]\n",
        "\n",
        "        # Update the GT index for these anchors to the best match\n",
        "        best_match_gt_idx[pred_inds_to_update] = best_match_gt_idx_pre_thresholding[pred_inds_to_update]\n",
        "\n",
        "        # Retrieve the matched ground truth boxes for each anchor (background anchors will be assigned the 0th GT box)\n",
        "        matched_gt_boxes = gt_boxes[best_match_gt_idx.clamp(min=0)]\n",
        "\n",
        "        # Set classification labels for anchors\n",
        "        labels = best_match_gt_idx >= 0  # foreground anchors (1)\n",
        "        labels = labels.to(dtype=torch.float32)\n",
        "\n",
        "        # Set background anchors (0)\n",
        "        background_anchors = best_match_gt_idx == -1\n",
        "        labels[background_anchors] = 0.0\n",
        "\n",
        "        # Set ignored anchors (-1)\n",
        "        ignored_anchors = best_match_gt_idx == -2\n",
        "        labels[ignored_anchors] = -1.0\n",
        "\n",
        "        return labels, matched_gt_boxes\n",
        "\n",
        "    def filter_proposals(self, proposals, cls_scores, image_shape):\n",
        "        \"\"\"\n",
        "        This method does three kinds of filtering/modifications\n",
        "        1. Pre NMS topK filtering\n",
        "        2. Make proposals valid by clamping coordinates(0, width/height)\n",
        "        2. Small Boxes filtering based on width and height\n",
        "        3. NMS\n",
        "        4. Post NMS topK filtering\n",
        "        :param proposals: (num_anchors_in_image, 4)\n",
        "        :param cls_scores: (num_anchors_in_image, 4) these are cls logits\n",
        "        :param image_shape: resized image shape needed to clip proposals to image boundary\n",
        "        :return: proposals and cls_scores: (num_filtered_proposals, 4) and (num_filtered_proposals)\n",
        "        \"\"\"\n",
        "        # Pre NMS Filtering\n",
        "        cls_scores = cls_scores.reshape(-1)\n",
        "        cls_scores = torch.sigmoid(cls_scores)\n",
        "        _, top_n_idx = cls_scores.topk(min(self.rpn_prenms_topk, len(cls_scores)))\n",
        "\n",
        "        cls_scores = cls_scores[top_n_idx]\n",
        "        proposals = proposals[top_n_idx]\n",
        "\n",
        "        # Clamp boxes to image boundary\n",
        "        proposals = clamp_boxes_to_image_boundary(proposals, image_shape)\n",
        "\n",
        "        # Small boxes based on width and height filtering\n",
        "        min_size = 16\n",
        "        ws, hs = proposals[:, 2] - proposals[:, 0], proposals[:, 3] - proposals[:, 1]\n",
        "        keep = (ws >= min_size) & (hs >= min_size)\n",
        "        keep = torch.where(keep)[0]\n",
        "        proposals = proposals[keep]\n",
        "        cls_scores = cls_scores[keep]\n",
        "\n",
        "        # NMS based on objectness scores\n",
        "        keep_mask = torch.zeros_like(cls_scores, dtype=torch.bool)\n",
        "        keep_indices = torch.ops.torchvision.nms(proposals, cls_scores, self.rpn_nms_threshold)\n",
        "        keep_mask[keep_indices] = True\n",
        "        keep_indices = torch.where(keep_mask)[0]\n",
        "        # Sort by objectness\n",
        "        post_nms_keep_indices = keep_indices[cls_scores[keep_indices].sort(descending=True)[1]]\n",
        "\n",
        "        # Post NMS topk filtering\n",
        "        proposals, cls_scores = (proposals[post_nms_keep_indices[:self.rpn_topk]],\n",
        "                                 cls_scores[post_nms_keep_indices[:self.rpn_topk]])\n",
        "\n",
        "        return proposals, cls_scores\n",
        "\n",
        "    def forward(self, image, feat, target=None):\n",
        "        \"\"\"\n",
        "        Main method for RPN does the following:\n",
        "        1. Call RPN specific conv layers to generate classification and\n",
        "            bbox transformation predictions for anchors\n",
        "        2. Generate anchors for entire image\n",
        "        3. Transform generated anchors based on predicted bbox transformation to generate proposals\n",
        "        4. Filter proposals\n",
        "        5. For training additionally we do the following:\n",
        "            a. Assign target ground truth labels and boxes to each anchors\n",
        "            b. Sample positive and negative anchors\n",
        "            c. Compute classification loss using sampled pos/neg anchors\n",
        "            d. Compute Localization loss using sampled pos anchors\n",
        "        :param image:\n",
        "        :param feat:\n",
        "        :param target:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Call RPN layers\n",
        "        rpn_feat = nn.ReLU()(self.rpn_conv(feat))\n",
        "        cls_scores = self.cls_layer(rpn_feat)\n",
        "        box_transform_pred = self.bbox_reg_layer(rpn_feat)\n",
        "\n",
        "        # Generate anchors\n",
        "        anchors = self.generate_anchors(image, feat)\n",
        "\n",
        "        # Reshape classification scores to be (Batch Size * H_feat * W_feat * Number of Anchors Per Location, 1)\n",
        "        # cls_score -> (Batch_Size, Number of Anchors per location, H_feat, W_feat)\n",
        "        number_of_anchors_per_location = cls_scores.size(1)\n",
        "        cls_scores = cls_scores.permute(0, 2, 3, 1)\n",
        "        cls_scores = cls_scores.reshape(-1, 1)\n",
        "        # cls_score -> (Batch_Size*H_feat*W_feat*Number of Anchors per location, 1)\n",
        "\n",
        "        # Reshape bbox predictions to be (Batch Size * H_feat * W_feat * Number of Anchors Per Location, 4)\n",
        "        # box_transform_pred -> (Batch_Size, Number of Anchors per location*4, H_feat, W_feat)\n",
        "        box_transform_pred = box_transform_pred.view(\n",
        "            box_transform_pred.size(0),\n",
        "            number_of_anchors_per_location,\n",
        "            4,\n",
        "            rpn_feat.shape[-2],\n",
        "            rpn_feat.shape[-1])\n",
        "        box_transform_pred = box_transform_pred.permute(0, 3, 4, 1, 2)\n",
        "        box_transform_pred = box_transform_pred.reshape(-1, 4)\n",
        "        # box_transform_pred -> (Batch_Size*H_feat*W_feat*Number of Anchors per location, 4)\n",
        "\n",
        "        # Transform generated anchors according to box transformation prediction\n",
        "        proposals = apply_regression_pred_to_anchors(\n",
        "            box_transform_pred.detach().reshape(-1, 1, 4),\n",
        "            anchors)\n",
        "        proposals = proposals.reshape(proposals.size(0), 4)\n",
        "\n",
        "        proposals, scores = self.filter_proposals(proposals, cls_scores.detach(), image.shape)\n",
        "        rpn_output = {\n",
        "            'proposals': proposals,\n",
        "            'scores': scores\n",
        "        }\n",
        "        if not self.training or target is None:\n",
        "            # If we are not training no need to do anything\n",
        "            return rpn_output\n",
        "        else:\n",
        "            # Assign gt box and label for each anchor\n",
        "            labels_for_anchors, matched_gt_boxes_for_anchors = self.assign_targets_to_anchors(\n",
        "                anchors,\n",
        "                target['bboxes'][0])\n",
        "\n",
        "            # Based on gt assignment above, get regression target for the anchors\n",
        "            # matched_gt_boxes_for_anchors -> (Number of anchors in image, 4)\n",
        "            # anchors -> (Number of anchors in image, 4)\n",
        "            regression_targets = boxes_to_transformation_targets(matched_gt_boxes_for_anchors, anchors)\n",
        "\n",
        "            ####### Sampling positive and negative anchors ####\n",
        "            # Our labels were {fg:1, bg:0, to_be_ignored:-1}\n",
        "            sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(\n",
        "                labels_for_anchors,\n",
        "                positive_count=self.rpn_pos_count,\n",
        "                total_count=self.rpn_batch_size)\n",
        "\n",
        "            sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n",
        "\n",
        "            localization_loss = (\n",
        "                    torch.nn.functional.smooth_l1_loss(\n",
        "                        box_transform_pred[sampled_pos_idx_mask],\n",
        "                        regression_targets[sampled_pos_idx_mask],\n",
        "                        beta=1/9,\n",
        "                        reduction=\"sum\",\n",
        "                    )\n",
        "                    / (sampled_idxs.numel())\n",
        "            )\n",
        "\n",
        "            cls_loss = torch.nn.functional.binary_cross_entropy_with_logits(cls_scores[sampled_idxs].flatten(),\n",
        "                                                                            labels_for_anchors[sampled_idxs].flatten())\n",
        "\n",
        "            rpn_output['rpn_class_loss'] = cls_loss\n",
        "            rpn_output['rpn_local_loss'] = localization_loss\n",
        "            return rpn_output\n",
        "\n",
        "\n",
        "class ROIHead(nn.Module):\n",
        "    \"\"\"\n",
        "    ROI head on top of ROI pooling layer for generating\n",
        "    classification and box transformation predictions\n",
        "    We have two fc layers followed by a classification fc layer\n",
        "    and a bbox regression fc layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, in_channels=512):\n",
        "        super(ROIHead, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.roi_batch_size = 2\n",
        "        self.roi_pos_count = int(1*self.roi_batch_size)\n",
        "        self.iou_threshold = 0.85\n",
        "        self.low_bg_iou = 0.1\n",
        "        self.nms_threshold = 0.2\n",
        "        self.topK_detections = 15\n",
        "        self.low_score_threshold = 0.05\n",
        "        self.pool_size = 7\n",
        "        self.fc_inner_dim = 1024\n",
        "\n",
        "        self.fc6 = nn.Linear(in_channels * self.pool_size * self.pool_size, self.fc_inner_dim)\n",
        "        self.fc7 = nn.Linear(self.fc_inner_dim, self.fc_inner_dim)\n",
        "        self.cls_layer = nn.Linear(self.fc_inner_dim, self.num_classes)\n",
        "        self.bbox_reg_layer = nn.Linear(self.fc_inner_dim, self.num_classes * 4)\n",
        "\n",
        "        torch.nn.init.normal_(self.cls_layer.weight, std=0.01)\n",
        "        torch.nn.init.constant_(self.cls_layer.bias, 0)\n",
        "\n",
        "        torch.nn.init.normal_(self.bbox_reg_layer.weight, std=0.001)\n",
        "        torch.nn.init.constant_(self.bbox_reg_layer.bias, 0)\n",
        "\n",
        "    def assign_target_to_proposals(self, proposals, gt_boxes, gt_labels):\n",
        "        \"\"\"\n",
        "        Assign ground truth boxes to proposals based on IOU, and generate classification labels.\n",
        "        - Labels are assigned as follows:\n",
        "            - 1 for proposals matching a ground truth box.\n",
        "            - 0 for proposals classified as background.\n",
        "            - -1 for proposals to be ignored.\n",
        "\n",
        "        :param proposals: (number_of_proposals, 4) Tensor of proposed box coordinates.\n",
        "        :param gt_boxes: (number_of_gt_boxes, 4) Tensor of ground truth box coordinates.\n",
        "        :param gt_labels: (number_of_gt_boxes) Tensor of class labels for each ground truth box.\n",
        "        :return:\n",
        "            labels: (number_of_proposals) Tensor of labels {-1, 0, 1} for each proposal.\n",
        "            matched_gt_boxes: (number_of_proposals, 4) Tensor of assigned ground truth box coordinates for each proposal.\n",
        "        \"\"\"\n",
        "        # Compute IOU between proposals and ground truth boxes\n",
        "        iou_matrix = get_iou(gt_boxes, proposals)\n",
        "\n",
        "        # Find the best matching ground truth box for each proposal\n",
        "        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n",
        "\n",
        "        # Identify proposals with low IOU (background or ignored)\n",
        "        background_proposals = (best_match_iou < self.iou_threshold) & (best_match_iou >= self.low_bg_iou)\n",
        "        ignored_proposals = best_match_iou < self.low_bg_iou\n",
        "\n",
        "        # Mark low IOU proposals as background (-1) or ignored (-2)\n",
        "        best_match_gt_idx[background_proposals] = -1\n",
        "        best_match_gt_idx[ignored_proposals] = -2\n",
        "\n",
        "        # Assign ground truth boxes to proposals (even background proposals)\n",
        "        matched_gt_boxes_for_proposals = gt_boxes[best_match_gt_idx.clamp(min=0)]\n",
        "\n",
        "        # Assign class labels based on the best matched ground truth box\n",
        "        labels = gt_labels[best_match_gt_idx.clamp(min=0)].to(dtype=torch.int64)\n",
        "\n",
        "        # Set background proposals to label 0\n",
        "        labels[background_proposals] = 0\n",
        "\n",
        "        # Set ignored proposals to label -1\n",
        "        labels[ignored_proposals] = -1\n",
        "\n",
        "        return labels, matched_gt_boxes_for_proposals\n",
        "\n",
        "    def forward(self, feat, proposals, image_shape, target):\n",
        "        \"\"\"\n",
        "        Main method for ROI head that does the following:\n",
        "        1. If training assign target boxes and labels to all proposals\n",
        "        2. If training sample positive and negative proposals\n",
        "        3. If training get bbox transformation targets for all proposals based on assignments\n",
        "        4. Get ROI Pooled features for all proposals\n",
        "        5. Call fc6, fc7 and classification and bbox transformation fc layers\n",
        "        6. Compute classification and localization loss\n",
        "\n",
        "        :param feat:\n",
        "        :param proposals:\n",
        "        :param image_shape:\n",
        "        :param target:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if self.training and target is not None:\n",
        "            # Add ground truth to proposals\n",
        "            proposals = torch.cat([proposals, target['bboxes'][0]], dim=0)\n",
        "\n",
        "            gt_boxes = target['bboxes'][0]\n",
        "            gt_labels = target['labels'][0]\n",
        "\n",
        "            labels, matched_gt_boxes_for_proposals = self.assign_target_to_proposals(proposals, gt_boxes, gt_labels)\n",
        "\n",
        "            sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(labels,\n",
        "                                                                                  positive_count=self.roi_pos_count,\n",
        "                                                                                  total_count=self.roi_batch_size)\n",
        "\n",
        "            sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n",
        "\n",
        "            # Keep only sampled proposals\n",
        "            proposals = proposals[sampled_idxs]\n",
        "            labels = labels[sampled_idxs]\n",
        "            matched_gt_boxes_for_proposals = matched_gt_boxes_for_proposals[sampled_idxs]\n",
        "            regression_targets = boxes_to_transformation_targets(matched_gt_boxes_for_proposals, proposals)\n",
        "            # regression_targets -> (sampled_training_proposals, 4)\n",
        "            # matched_gt_boxes_for_proposals -> (sampled_training_proposals, 4)\n",
        "\n",
        "        # Get desired scale to pass to roi pooling function\n",
        "        size = feat.shape[-2:]\n",
        "        possible_scales = []\n",
        "        for s1, s2 in zip(size, image_shape):\n",
        "            approx_scale = float(s1) / float(s2)\n",
        "            scale = 2 ** float(torch.tensor(approx_scale).log2().round())\n",
        "            possible_scales.append(scale)\n",
        "        assert possible_scales[0] == possible_scales[1]\n",
        "\n",
        "        # ROI pooling and call all layers for prediction\n",
        "        proposal_roi_pool_feats = torchvision.ops.roi_pool(feat, [proposals],\n",
        "                                                           output_size=self.pool_size,\n",
        "                                                           spatial_scale=possible_scales[0])\n",
        "        proposal_roi_pool_feats = proposal_roi_pool_feats.flatten(start_dim=1)\n",
        "        box_fc_6 = torch.nn.functional.relu(self.fc6(proposal_roi_pool_feats))\n",
        "        box_fc_7 = torch.nn.functional.relu(self.fc7(box_fc_6))\n",
        "        cls_scores = self.cls_layer(box_fc_7)\n",
        "        box_transform_pred = self.bbox_reg_layer(box_fc_7)\n",
        "        # cls_scores -> (proposals, num_classes)\n",
        "        # box_transform_pred -> (proposals, num_classes * 4)\n",
        "\n",
        "        num_boxes, num_classes = cls_scores.shape\n",
        "        box_transform_pred = box_transform_pred.reshape(num_boxes, num_classes, 4)\n",
        "        frcnn_output = {}\n",
        "        if self.training and target is not None:\n",
        "            classification_loss = torch.nn.functional.cross_entropy(cls_scores, labels)\n",
        "\n",
        "            # Compute localization loss only for non-background labelled proposals\n",
        "            fg_proposals_idxs = torch.where(labels > 0)[0]\n",
        "            # Get class labels for these positive proposals\n",
        "            fg_cls_labels = labels[fg_proposals_idxs]\n",
        "\n",
        "            localization_loss = torch.nn.functional.smooth_l1_loss(\n",
        "                box_transform_pred[fg_proposals_idxs, fg_cls_labels],\n",
        "                regression_targets[fg_proposals_idxs],\n",
        "                beta=1/9,\n",
        "                reduction=\"sum\",\n",
        "            )\n",
        "            localization_loss = localization_loss / labels.numel()\n",
        "            frcnn_output['frcnn_class_loss'] = classification_loss\n",
        "            frcnn_output['frcnn_local_loss'] = localization_loss\n",
        "\n",
        "        if self.training:\n",
        "            return frcnn_output\n",
        "        else:\n",
        "            device = cls_scores.device\n",
        "            # Apply transformation predictions to proposals\n",
        "            pred_boxes = apply_regression_pred_to_anchors(box_transform_pred, proposals)\n",
        "            pred_scores = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "\n",
        "            # Clamp box to image boundary\n",
        "            pred_boxes = clamp_boxes_to_image_boundary(pred_boxes, image_shape)\n",
        "\n",
        "            # create labels for each prediction\n",
        "            pred_labels = torch.arange(num_classes, device=device)\n",
        "            pred_labels = pred_labels.view(1, -1).expand_as(pred_scores)\n",
        "\n",
        "            # remove predictions with the background label\n",
        "            pred_boxes = pred_boxes[:, 1:]\n",
        "            pred_scores = pred_scores[:, 1:]\n",
        "            pred_labels = pred_labels[:, 1:]\n",
        "\n",
        "            # pred_boxes -> (number_proposals, num_classes-1, 4)\n",
        "            # pred_scores -> (number_proposals, num_classes-1)\n",
        "            # pred_labels -> (number_proposals, num_classes-1)\n",
        "\n",
        "            # batch everything, by making every class prediction be a separate instance\n",
        "            pred_boxes = pred_boxes.reshape(-1, 4)\n",
        "            pred_scores = pred_scores.reshape(-1)\n",
        "            pred_labels = pred_labels.reshape(-1)\n",
        "\n",
        "            pred_boxes, pred_labels, pred_scores = self.filter_predictions(pred_boxes, pred_labels, pred_scores)\n",
        "            frcnn_output['boxes'] = pred_boxes\n",
        "            frcnn_output['scores'] = pred_scores\n",
        "            frcnn_output['labels'] = pred_labels\n",
        "            return frcnn_output\n",
        "\n",
        "    def filter_predictions(self, pred_boxes, pred_labels, pred_scores):\n",
        "        \"\"\"\n",
        "        Method to filter predictions by applying the following in order:\n",
        "        1. Filter low scoring boxes\n",
        "        2. Remove small size boxes\n",
        "        3. NMS for each class separately\n",
        "        4. Keep only topK detections\n",
        "        :param pred_boxes:\n",
        "        :param pred_labels:\n",
        "        :param pred_scores:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # remove low scoring boxes\n",
        "        keep = torch.where(pred_scores > self.low_score_threshold)[0]\n",
        "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n",
        "\n",
        "        # Remove small boxes\n",
        "        min_size = 16\n",
        "        ws, hs = pred_boxes[:, 2] - pred_boxes[:, 0], pred_boxes[:, 3] - pred_boxes[:, 1]\n",
        "        keep = (ws >= min_size) & (hs >= min_size)\n",
        "        keep = torch.where(keep)[0]\n",
        "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n",
        "\n",
        "        # Class wise nms\n",
        "        keep_mask = torch.zeros_like(pred_scores, dtype=torch.bool)\n",
        "        for class_id in torch.unique(pred_labels):\n",
        "            curr_indices = torch.where(pred_labels == class_id)[0]\n",
        "            curr_keep_indices = torch.ops.torchvision.nms(pred_boxes[curr_indices],\n",
        "                                                          pred_scores[curr_indices],\n",
        "                                                          self.nms_threshold)\n",
        "            keep_mask[curr_indices[curr_keep_indices]] = True\n",
        "        keep_indices = torch.where(keep_mask)[0]\n",
        "        post_nms_keep_indices = keep_indices[pred_scores[keep_indices].sort(descending=True)[1]]\n",
        "        keep = post_nms_keep_indices[:self.topK_detections]\n",
        "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n",
        "        return pred_boxes, pred_labels, pred_scores\n",
        "\n",
        "\n",
        "class FasterRCNN(nn.Module):\n",
        "    def __init__(self, num_classes, scales, aspect_ratios):\n",
        "        super(FasterRCNN, self).__init__()\n",
        "        vgg16 = torchvision.models.vgg16(pretrained=True)\n",
        "        self.backbone = vgg16.features[:-1]\n",
        "        self.rpn = RegionProposalNetwork(512,\n",
        "                                         scales=[128, 256, 512],\n",
        "                                         aspect_ratios=[0.5, 1, 2])\n",
        "        self.roi_head = ROIHead(num_classes, in_channels=512)\n",
        "        for layer in self.backbone[:10]:\n",
        "            for p in layer.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "    def forward(self, image, target=None):\n",
        "        old_shape = image.shape[-2:]\n",
        "\n",
        "        # Call backbone\n",
        "        feat = self.backbone(image)\n",
        "\n",
        "        # Call RPN and get proposals\n",
        "        rpn_output = self.rpn(image, feat, target)\n",
        "        proposals = rpn_output['proposals']\n",
        "\n",
        "        # Call ROI head and convert proposals to boxes\n",
        "        frcnn_output = self.roi_head(feat, proposals, image.shape[-2:], target)\n",
        "\n",
        "        return rpn_output, frcnn_output"
      ],
      "metadata": {
        "id": "endSQwJ0nVhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Code tools"
      ],
      "metadata": {
        "id": "vyJLb_aZY8x-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_info(image_directory, annotation_directory, label2idx):\n",
        "    im_infos = []\n",
        "\n",
        "    # Iterate over all files in the image directory\n",
        "    for filename in os.listdir(image_directory):\n",
        "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "            img_id = filename.split('.')[0]\n",
        "            img_path = os.path.join(image_directory, filename)\n",
        "\n",
        "            # Read image to get dimensions\n",
        "            image = cv2.imread(img_path)\n",
        "            height, width, _ = image.shape\n",
        "\n",
        "            # Initialize detections list\n",
        "            detections = []\n",
        "\n",
        "            # Read corresponding annotation file\n",
        "            annotation_file = os.path.join(annotation_directory, f\"{img_id}.txt\")\n",
        "            if os.path.exists(annotation_file):\n",
        "                with open(annotation_file, 'r') as file:\n",
        "                    for line in file:\n",
        "                        parts = line.strip().split()\n",
        "                        if len(parts) == 5:\n",
        "                            class_id = int(parts[0])  # Original class ID from annotation\n",
        "\n",
        "                            # Map class ID to label index\n",
        "                            if class_id in range(len(label2idx) - 1):  # Check if class_id is valid\n",
        "                                mapped_label = label2idx[classes[class_id + 1]]  # Adjust index for 'background'\n",
        "\n",
        "                                x_center = float(parts[1]) * width\n",
        "                                y_center = float(parts[2]) * height\n",
        "                                box_width = float(parts[3]) * width\n",
        "                                box_height = float(parts[4]) * height\n",
        "\n",
        "                                # Calculate bounding box coordinates\n",
        "                                x_min = int(x_center - box_width / 2)\n",
        "                                y_min = int(y_center - box_height / 2)\n",
        "                                x_max = int(x_center + box_width / 2)\n",
        "                                y_max = int(y_center + box_height / 2)\n",
        "\n",
        "                                # Add detection with mapped label\n",
        "                                detections.append({\n",
        "                                    'label': mapped_label,\n",
        "                                    'bbox': [x_min, y_min, x_max, y_max]\n",
        "                                })\n",
        "\n",
        "            # Append image info to the list\n",
        "            im_info = {\n",
        "                'img_id': img_id,\n",
        "                'filename': img_path,\n",
        "                'width': width,\n",
        "                'height': height,\n",
        "                'detections': detections\n",
        "            }\n",
        "            im_infos.append(im_info)\n",
        "\n",
        "    return im_infos"
      ],
      "metadata": {
        "id": "_60whZjs4hx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im_dir = '/content/drive/MyDrive/Colab Notebooks/ir_images'\n",
        "label_dir = '/content/drive/MyDrive/Colab Notebooks/ir_labels'\n",
        "\n",
        "\n",
        "classes = ['person', 'car']\n",
        "classes = ['background'] + classes  # Background is index 0\n",
        "label2idx = {classes[idx]: idx for idx in range(len(classes))}\n",
        "idx2label = {idx: classes[idx] for idx in range(len(classes))}\n",
        "print(label2idx)\n",
        "\n",
        "get_image_info(im_dir, label_dir, label2idx)"
      ],
      "metadata": {
        "id": "M146E4mdcdqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18900107-4be7-4a8a-f84d-c12c23adcc1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'background': 0, 'person': 1, 'car': 2}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'img_id': 'video_frame_001061',\n",
              "  'filename': '/content/drive/MyDrive/Colab Notebooks/ir_images/video_frame_001061.jpg',\n",
              "  'width': 640,\n",
              "  'height': 512,\n",
              "  'detections': [{'label': 1, 'bbox': [89, 248, 108, 303]},\n",
              "   {'label': 1, 'bbox': [404, 249, 422, 323]},\n",
              "   {'label': 1, 'bbox': [390, 257, 409, 316]},\n",
              "   {'label': 1, 'bbox': [468, 284, 482, 309]},\n",
              "   {'label': 1, 'bbox': [573, 266, 597, 326]},\n",
              "   {'label': 1, 'bbox': [609, 251, 633, 325]}]},\n",
              " {'img_id': 'video_frame_002122',\n",
              "  'filename': '/content/drive/MyDrive/Colab Notebooks/ir_images/video_frame_002122.jpg',\n",
              "  'width': 640,\n",
              "  'height': 512,\n",
              "  'detections': [{'label': 1, 'bbox': [65, 248, 94, 303]},\n",
              "   {'label': 1, 'bbox': [109, 259, 126, 302]}]},\n",
              " {'img_id': 'video_frame_000001',\n",
              "  'filename': '/content/drive/MyDrive/Colab Notebooks/ir_images/video_frame_000001.jpg',\n",
              "  'width': 640,\n",
              "  'height': 512,\n",
              "  'detections': [{'label': 1, 'bbox': [147, 254, 164, 304]},\n",
              "   {'label': 1, 'bbox': [571, 267, 588, 318]},\n",
              "   {'label': 1, 'bbox': [468, 282, 482, 308]}]},\n",
              " {'img_id': 'video_frame_009544',\n",
              "  'filename': '/content/drive/MyDrive/Colab Notebooks/ir_images/video_frame_009544.jpg',\n",
              "  'width': 640,\n",
              "  'height': 512,\n",
              "  'detections': [{'label': 1, 'bbox': [355, 256, 372, 318]},\n",
              "   {'label': 2, 'bbox': [0, 184, 111, 309]}]},\n",
              " {'img_id': 'video_frame_007423',\n",
              "  'filename': '/content/drive/MyDrive/Colab Notebooks/ir_images/video_frame_007423.jpg',\n",
              "  'width': 640,\n",
              "  'height': 512,\n",
              "  'detections': [{'label': 1, 'bbox': [61, 254, 76, 300]},\n",
              "   {'label': 1, 'bbox': [279, 249, 300, 314]},\n",
              "   {'label': 2, 'bbox': [475, 243, 639, 345]}]},\n",
              " {'img_id': 'video_frame_005302',\n",
              "  'filename': '/content/drive/MyDrive/Colab Notebooks/ir_images/video_frame_005302.jpg',\n",
              "  'width': 640,\n",
              "  'height': 512,\n",
              "  'detections': [{'label': 1, 'bbox': [10, 251, 31, 300]}]},\n",
              " {'img_id': 'video_frame_003182',\n",
              "  'filename': '/content/drive/MyDrive/Colab Notebooks/ir_images/video_frame_003182.jpg',\n",
              "  'width': 640,\n",
              "  'height': 512,\n",
              "  'detections': [{'label': 1, 'bbox': [355, 259, 380, 318]},\n",
              "   {'label': 1, 'bbox': [376, 255, 408, 319]},\n",
              "   {'label': 2, 'bbox': [45, 237, 251, 322]}]},\n",
              " {'img_id': 'video_frame_004242',\n",
              "  'filename': '/content/drive/MyDrive/Colab Notebooks/ir_images/video_frame_004242.jpg',\n",
              "  'width': 640,\n",
              "  'height': 512,\n",
              "  'detections': [{'label': 1, 'bbox': [572, 259, 597, 323]}]},\n",
              " {'img_id': 'video_frame_006363',\n",
              "  'filename': '/content/drive/MyDrive/Colab Notebooks/ir_images/video_frame_006363.jpg',\n",
              "  'width': 640,\n",
              "  'height': 512,\n",
              "  'detections': [{'label': 1, 'bbox': [355, 254, 370, 285]},\n",
              "   {'label': 1, 'bbox': [601, 252, 626, 329]},\n",
              "   {'label': 1, 'bbox': [578, 274, 600, 323]},\n",
              "   {'label': 2, 'bbox': [6, 257, 140, 316]},\n",
              "   {'label': 2, 'bbox': [331, 262, 544, 339]}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Analysis Tools"
      ],
      "metadata": {
        "id": "8xRe8vVY0pGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IRDataset(Dataset):\n",
        "    def __init__(self, split, im_dir, ann_dir, split_ratio=0.8):\n",
        "        self.split = split\n",
        "        self.im_dir = im_dir\n",
        "        self.ann_dir = ann_dir\n",
        "\n",
        "        # Define the classes\n",
        "        classes = ['person', 'car']\n",
        "        classes = sorted(classes)\n",
        "        classes = ['background'] + classes  # Background is index 0\n",
        "\n",
        "        self.label2idx = {classes[idx]: idx for idx in range(len(classes))}\n",
        "        self.idx2label = {idx: classes[idx] for idx in range(len(classes))}\n",
        "        print(self.idx2label)\n",
        "\n",
        "        # Use the function to load image information\n",
        "        self.images_info = get_image_info(im_dir, ann_dir, self.label2idx)\n",
        "\n",
        "        '''# Split the dataset into training and validation\n",
        "        split_index = int(len(self.images_info) * split_ratio)\n",
        "        if split == 'train':\n",
        "            self.images_info = self.images_info[:split_index]\n",
        "        elif split == 'val':\n",
        "            self.images_info = self.images_info[split_index:]\n",
        "        else:\n",
        "            raise ValueError(\"Split must be either 'train' or 'val'\")'''\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_info)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        im_info = self.images_info[index]\n",
        "        im = Image.open(im_info['filename'])\n",
        "\n",
        "        im_tensor = torchvision.transforms.ToTensor()(im)\n",
        "\n",
        "        targets = {}\n",
        "        targets['bboxes'] = torch.as_tensor([detection['bbox'] for detection in im_info['detections']], dtype=torch.float32)\n",
        "        targets['labels'] = torch.as_tensor([detection['label'] for detection in im_info['detections']], dtype=torch.int64)\n",
        "\n",
        "        return im_tensor, targets, im_info['filename']"
      ],
      "metadata": {
        "id": "y0x56DIt5cyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = IRDataset('train', im_dir, label_dir)\n",
        "val_dataset = IRDataset('val', im_dir, label_dir)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f'Training dataset size: {len(train_dataset)}')\n",
        "print(f'Validation dataset size: {len(val_dataset)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jts4qxIW5ieG",
        "outputId": "5ca18bbf-23c0-4de0-bee9-c2b7e00f6910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'background', 1: 'car', 2: 'person'}\n",
            "{0: 'background', 1: 'car', 2: 'person'}\n",
            "Training dataset size: 9\n",
            "Validation dataset size: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "faster_rcnn_model = faster_rcnn_model = FasterRCNN(num_classes=3, scales=[64, 128, 256], aspect_ratios=[0.5, 1, 2])\n",
        "\n",
        "\n",
        "train_dataset = IRDataset('train', im_dir='/content/drive/MyDrive/Colab Notebooks/ir_images',\n",
        "                  ann_dir='/content/drive/MyDrive/Colab Notebooks/ir_labels')\n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                            batch_size=1,\n",
        "                            shuffle=True,\n",
        "                            num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0ZVULk8_BQF",
        "outputId": "01392e06-7915-4709-be56-5df97d6bfd13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:06<00:00, 87.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'background', 1: 'car', 2: 'person'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, custom_dataset, train_loader):\n",
        "\n",
        "    seed = 42\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    if not os.path.exists('frcnn'):\n",
        "        os.mkdir('frcnn')\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    acc_steps = 1\n",
        "    num_epochs =5\n",
        "    step_count = 1\n",
        "\n",
        "    # Lists to store losses\n",
        "    rpn_classification_loss_history = []\n",
        "    rpn_localization_loss_history = []\n",
        "    frcnn_classification_loss_history = []\n",
        "    frcnn_localization_loss_history = []\n",
        "    mean_ap_history = []\n",
        "    all_aps_history = []\n",
        "\n",
        "    gts = []\n",
        "    preds = []\n",
        "\n",
        "    for i in range(num_epochs):\n",
        "        rpn_classification_losses = []\n",
        "        rpn_localization_losses = []\n",
        "        frcnn_classification_losses = []\n",
        "        frcnn_localization_losses = []\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for im, target, fname in tqdm(train_loader):\n",
        "            im = im.float().to(device)\n",
        "\n",
        "            #target_boxes = target['bboxes'].float().to(device)[0]\n",
        "            #target_labels = target['labels'].long().to(device)[0]\n",
        "\n",
        "            target['bboxes'] = target['bboxes'].float().to(device)\n",
        "            target['labels'] = target['labels'].long().to(device)\n",
        "            rpn_output, frcnn_output = model(im, target)\n",
        "\n",
        "            rpn_loss = rpn_output['rpn_class_loss'] + rpn_output['rpn_local_loss']\n",
        "            frcnn_loss = frcnn_output['frcnn_class_loss'] + frcnn_output['frcnn_local_loss']\n",
        "            loss = rpn_loss + frcnn_loss\n",
        "\n",
        "            rpn_classification_losses.append(rpn_output['rpn_class_loss'].item())\n",
        "            rpn_localization_losses.append(rpn_output['rpn_local_loss'].item())\n",
        "            frcnn_classification_losses.append(frcnn_output['frcnn_class_loss'].item())\n",
        "            frcnn_localization_losses.append(frcnn_output['frcnn_local_loss'].item())\n",
        "            loss = loss / acc_steps\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            step_count += 1\n",
        "\n",
        "            '''# Evaluate MAP\n",
        "            model.eval()\n",
        "            rpn_output, frcnn_output = model(im, None)\n",
        "            boxes = frcnn_output['boxes']\n",
        "            labels = frcnn_output['labels']\n",
        "            scores = frcnn_output['scores']\n",
        "\n",
        "            pred_boxes = {}\n",
        "            gt_boxes = {}\n",
        "            for label_name in custom_dataset.label2idx:\n",
        "                pred_boxes[label_name] = []\n",
        "                gt_boxes[label_name] = []\n",
        "\n",
        "            for idx, box in enumerate(boxes):\n",
        "                x1, y1, x2, y2 = box.detach().cpu().numpy()\n",
        "                label = labels[idx].detach().cpu().item()\n",
        "                score = scores[idx].detach().cpu().item()\n",
        "                label_name = custom_dataset.idx2label[label]\n",
        "                pred_boxes[label_name].append([x1, y1, x2, y2, score])\n",
        "            for idx, box in enumerate(target_boxes):\n",
        "                x1, y1, x2, y2 = box.detach().cpu().numpy()\n",
        "                label = target_labels[idx].detach().cpu().item()\n",
        "                label_name = custom_dataset.idx2label[label]\n",
        "                gt_boxes[label_name].append([x1, y1, x2, y2])\n",
        "\n",
        "            gts.append(gt_boxes)\n",
        "            preds.append(pred_boxes)\n",
        "            model.train()\n",
        "        mean_ap, all_aps = compute_map(preds, gts)\n",
        "        print('Class Wise Average Precisions')\n",
        "        for idx in range(len(custom_dataset.idx2label)):\n",
        "            print('AP for class {} = {:.4f}'.format(custom_dataset.idx2label[idx], all_aps[custom_dataset.idx2label[idx]]))\n",
        "        print('Mean Average Precision : {:.4f}'.format(mean_ap))'''\n",
        "\n",
        "        print('Finished epoch {}'.format(i))\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calculate average losses for the epoch\n",
        "        avg_rpn_classification_loss = np.mean(rpn_classification_losses)\n",
        "        avg_rpn_localization_loss = np.mean(rpn_localization_losses)\n",
        "        avg_frcnn_classification_loss = np.mean(frcnn_classification_losses)\n",
        "        avg_frcnn_localization_loss = np.mean(frcnn_localization_losses)\n",
        "\n",
        "        # Store the average losses\n",
        "        rpn_classification_loss_history.append(avg_rpn_classification_loss)\n",
        "        rpn_localization_loss_history.append(avg_rpn_localization_loss)\n",
        "        frcnn_classification_loss_history.append(avg_frcnn_classification_loss)\n",
        "        frcnn_localization_loss_history.append(avg_frcnn_localization_loss)\n",
        "\n",
        "        # STore mAP\n",
        "        #mean_ap_history.append(mean_ap)\n",
        "        #all_aps_history.append(all_aps)\n",
        "\n",
        "        torch.save(model.state_dict(), os.path.join('frcnn',\n",
        "                                                                'save'))\n",
        "        loss_output = ''\n",
        "        loss_output += 'RPN Classification Loss : {:.4f}'.format(np.mean(rpn_classification_losses))\n",
        "        loss_output += ' | RPN Localization Loss : {:.4f}'.format(np.mean(rpn_localization_losses))\n",
        "        loss_output += ' | FRCNN Classification Loss : {:.4f}'.format(np.mean(frcnn_classification_losses))\n",
        "        loss_output += ' | FRCNN Localization Loss : {:.4f}'.format(np.mean(frcnn_localization_losses))\n",
        "        print(loss_output)\n",
        "\n",
        "    print('Done Training...')\n",
        "    return rpn_classification_loss_history, rpn_localization_loss_history, frcnn_classification_loss_history, frcnn_localization_loss_history, mean_ap_history"
      ],
      "metadata": {
        "id": "mcuHawAD6_6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faster_rcnn_model = faster_rcnn_model = FasterRCNN(num_classes=3, scales=[128, 256, 512], aspect_ratios=[0.5, 1, 2])\n",
        "\n",
        "\n",
        "train_dataset = IRDataset('train', im_dir='/content/drive/MyDrive/Colab Notebooks/ir_images',\n",
        "                  ann_dir='/content/drive/MyDrive/Colab Notebooks/ir_labels')\n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                            batch_size=1,\n",
        "                            shuffle=True,\n",
        "                            num_workers=4)\n",
        "\n",
        "train(faster_rcnn_model, train_dataset, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CH_zwS4x841e",
        "outputId": "6eb2da4b-087d-44c9-8668-720ec6a20ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'background', 1: 'car', 2: 'person'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [01:32<00:00, 10.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished epoch 0\n",
            "RPN Classification Loss : 0.4531 | RPN Localization Loss : 2.9760 | FRCNN Classification Loss : 1.0524 | FRCNN Localization Loss : 0.4221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [01:31<00:00, 10.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished epoch 1\n",
            "RPN Classification Loss : 0.2390 | RPN Localization Loss : 2.0839 | FRCNN Classification Loss : 0.7899 | FRCNN Localization Loss : 0.5305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [01:34<00:00, 10.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished epoch 2\n",
            "RPN Classification Loss : 0.3071 | RPN Localization Loss : 3.2980 | FRCNN Classification Loss : 0.6061 | FRCNN Localization Loss : 0.1762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [01:31<00:00, 10.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished epoch 3\n",
            "RPN Classification Loss : 0.0842 | RPN Localization Loss : 2.0585 | FRCNN Classification Loss : 0.8099 | FRCNN Localization Loss : 0.0406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [01:31<00:00, 10.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished epoch 4\n",
            "RPN Classification Loss : 0.2071 | RPN Localization Loss : 1.8741 | FRCNN Classification Loss : 0.4688 | FRCNN Localization Loss : 0.3072\n",
            "Done Training...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.453055905074709,\n",
              "  0.2389744319435623,\n",
              "  0.3071127154980786,\n",
              "  0.08423384444581138,\n",
              "  0.20712732120106617],\n",
              " [2.976001309023963,\n",
              "  2.083935797214508,\n",
              "  3.2979558971193104,\n",
              "  2.0585126479466758,\n",
              "  1.8740801678763495],\n",
              " [1.0524442560142941,\n",
              "  0.7899185817546418,\n",
              "  0.6061433888971806,\n",
              "  0.8099084297815958,\n",
              "  0.46875843591988087],\n",
              " [0.4221344954914659,\n",
              "  0.530455916059307,\n",
              "  0.1761951064108871,\n",
              "  0.040600908855493695,\n",
              "  0.3072451932756748],\n",
              " [])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getter_iou(det, gt):\n",
        "    det_x1, det_y1, det_x2, det_y2 = det\n",
        "    gt_x1, gt_y1, gt_x2, gt_y2 = gt\n",
        "\n",
        "    x_left = max(det_x1, gt_x1)\n",
        "    y_top = max(det_y1, gt_y1)\n",
        "    x_right = min(det_x2, gt_x2)\n",
        "    y_bottom = min(det_y2, gt_y2)\n",
        "\n",
        "    if x_right < x_left or y_bottom < y_top:\n",
        "        return 0.0\n",
        "\n",
        "    area_intersection = (x_right - x_left) * (y_bottom - y_top)\n",
        "    det_area = (det_x2 - det_x1) * (det_y2 - det_y1)\n",
        "    gt_area = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)\n",
        "    area_union = float(det_area + gt_area - area_intersection + 1E-6)\n",
        "    iou = area_intersection / area_union\n",
        "    return iou\n",
        "\n",
        "def compute_map(det_boxes, gt_boxes, iou_threshold=0.7, method='area'):\n",
        "\n",
        "    gt_labels = {cls_key for im_gt in gt_boxes for cls_key in im_gt.keys()}\n",
        "    gt_labels = sorted(gt_labels)\n",
        "    all_aps = {}\n",
        "    # average precisions for ALL classes\n",
        "    aps = []\n",
        "    for idx, label in enumerate(gt_labels):\n",
        "        # Get detection predictions of this class\n",
        "        cls_dets = [\n",
        "            [im_idx, im_dets_label] for im_idx, im_dets in enumerate(det_boxes)\n",
        "            if label in im_dets for im_dets_label in im_dets[label]\n",
        "        ]\n",
        "\n",
        "        # Sort them by confidence score\n",
        "        cls_dets = sorted(cls_dets, key=lambda k: -k[1][-1])\n",
        "\n",
        "        # For tracking which gt boxes of this class have already been matched\n",
        "        gt_matched = [[False for _ in im_gts[label]] for im_gts in gt_boxes]\n",
        "        # Number of gt boxes for this class for recall calculation\n",
        "        num_gts = sum([len(im_gts[label]) for im_gts in gt_boxes])\n",
        "        tp = [0] * len(cls_dets)\n",
        "        fp = [0] * len(cls_dets)\n",
        "\n",
        "        # For each prediction\n",
        "        for det_idx, (im_idx, det_pred) in enumerate(cls_dets):\n",
        "            # Get gt boxes for this image and this label\n",
        "            im_gts = gt_boxes[im_idx][label]\n",
        "            max_iou_found = -1\n",
        "            max_iou_gt_idx = -1\n",
        "\n",
        "            # Get best matching gt box\n",
        "            for gt_box_idx, gt_box in enumerate(im_gts):\n",
        "                gt_box_iou = getter_iou(det_pred[:-1], gt_box)\n",
        "                if gt_box_iou > max_iou_found:\n",
        "                    max_iou_found = gt_box_iou\n",
        "                    max_iou_gt_idx = gt_box_idx\n",
        "            # TP only if iou >= threshold and this gt has not yet been matched\n",
        "            if max_iou_found < iou_threshold or gt_matched[im_idx][max_iou_gt_idx]:\n",
        "                fp[det_idx] = 1\n",
        "            else:\n",
        "                tp[det_idx] = 1\n",
        "                # If tp then we set this gt box as matched\n",
        "                gt_matched[im_idx][max_iou_gt_idx] = True\n",
        "        # Cumulative tp and fp\n",
        "        tp = np.cumsum(tp)\n",
        "        fp = np.cumsum(fp)\n",
        "\n",
        "        eps = np.finfo(np.float32).eps\n",
        "        recalls = tp / np.maximum(num_gts, eps)\n",
        "        precisions = tp / np.maximum((tp + fp), eps)\n",
        "\n",
        "        if method == 'area':\n",
        "            recalls = np.concatenate(([0.0], recalls, [1.0]))\n",
        "            precisions = np.concatenate(([0.0], precisions, [0.0]))\n",
        "\n",
        "            for i in range(precisions.size - 1, 0, -1):\n",
        "                precisions[i - 1] = np.maximum(precisions[i - 1], precisions[i])\n",
        "            # For computing area, get points where recall changes value\n",
        "            i = np.where(recalls[1:] != recalls[:-1])[0]\n",
        "            # Add the rectangular areas to get ap\n",
        "            ap = np.sum((recalls[i + 1] - recalls[i]) * precisions[i + 1])\n",
        "        elif method == 'interp':\n",
        "            ap = 0.0\n",
        "            for interp_pt in np.arange(0, 1 + 1E-3, 0.1):\n",
        "                # Get precision values for recall values >= interp_pt\n",
        "                prec_interp_pt = precisions[recalls >= interp_pt]\n",
        "\n",
        "                # Get max of those precision values\n",
        "                prec_interp_pt = prec_interp_pt.max() if prec_interp_pt.size > 0.0 else 0.0\n",
        "                ap += prec_interp_pt\n",
        "            ap = ap / 11.0\n",
        "        else:\n",
        "            raise ValueError('Method can only be area or interp')\n",
        "        if num_gts > 0:\n",
        "            aps.append(ap)\n",
        "            all_aps[label] = ap\n",
        "        else:\n",
        "            all_aps[label] = np.nan\n",
        "    # compute mAP at provided iou threshold\n",
        "    mean_ap = sum(aps) / len(aps)\n",
        "    return mean_ap, all_aps"
      ],
      "metadata": {
        "id": "HgaYLSLi6Qlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(model, custom_dataset):\n",
        "    model.eval()\n",
        "    if not os.path.exists('samples'):\n",
        "        os.mkdir('samples')\n",
        "\n",
        "    faster_rcnn_model.roi_head.low_score_threshold = 0.1\n",
        "\n",
        "    for sample_count in tqdm(range(1)):\n",
        "        random_idx = random.randint(0, len(custom_dataset))\n",
        "        im, target, fname = custom_dataset[random_idx]\n",
        "        im = im.unsqueeze(0).float().to(device)\n",
        "\n",
        "        gt_im = cv2.imread(fname)\n",
        "        gt_im_copy = gt_im.copy()\n",
        "\n",
        "        # Saving images with ground truth boxes\n",
        "        for idx, box in enumerate(target['bboxes']):\n",
        "            x1, y1, x2, y2 = box.detach().cpu().numpy()\n",
        "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "\n",
        "            cv2.rectangle(gt_im, (x1, y1), (x2, y2), thickness=2, color=[0, 255, 0])\n",
        "            cv2.rectangle(gt_im_copy, (x1, y1), (x2, y2), thickness=2, color=[0, 255, 0])\n",
        "            text = custom_dataset.idx2label[target['labels'][idx].detach().cpu().item()]\n",
        "            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, 1, 1)\n",
        "            text_w, text_h = text_size\n",
        "            cv2.rectangle(gt_im_copy , (x1, y1), (x1 + 10+text_w, y1 + 10+text_h), [255, 255, 255], -1)\n",
        "            cv2.putText(gt_im, text=custom_dataset.idx2label[target['labels'][idx].detach().cpu().item()],\n",
        "                        org=(x1+5, y1+15),\n",
        "                        thickness=1,\n",
        "                        fontScale=1,\n",
        "                        color=[0, 0, 0],\n",
        "                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n",
        "            cv2.putText(gt_im_copy, text=text,\n",
        "                        org=(x1 + 5, y1 + 15),\n",
        "                        thickness=1,\n",
        "                        fontScale=1,\n",
        "                        color=[0, 0, 0],\n",
        "                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n",
        "        cv2.addWeighted(gt_im_copy, 0.7, gt_im, 0.3, 0, gt_im)\n",
        "        cv2.imwrite('samples/output_frcnn_gt_{}.png'.format(sample_count), gt_im)\n",
        "\n",
        "        # Getting predictions from trained model\n",
        "        rpn_output, frcnn_output = model(im, None)\n",
        "        boxes = frcnn_output['boxes']\n",
        "        labels = frcnn_output['labels']\n",
        "        scores = frcnn_output['scores']\n",
        "        im = cv2.imread(fname)\n",
        "        im_copy = im.copy()\n",
        "\n",
        "        # Saving images with predicted boxes\n",
        "        for idx, box in enumerate(boxes):\n",
        "            x1, y1, x2, y2 = box.detach().cpu().numpy()\n",
        "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "            cv2.rectangle(im, (x1, y1), (x2, y2), thickness=2, color=[0, 0, 255])\n",
        "            cv2.rectangle(im_copy, (x1, y1), (x2, y2), thickness=2, color=[0, 0, 255])\n",
        "            text = '{} : {:.2f}'.format(custom_dataset.idx2label[labels[idx].detach().cpu().item()],\n",
        "                                        scores[idx].detach().cpu().item())\n",
        "            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, 1, 1)\n",
        "            text_w, text_h = text_size\n",
        "            cv2.rectangle(im_copy , (x1, y1), (x1 + 10+text_w, y1 + 10+text_h), [255, 255, 255], -1)\n",
        "            cv2.putText(im, text=text,\n",
        "                        org=(x1+5, y1+15),\n",
        "                        thickness=1,\n",
        "                        fontScale=1,\n",
        "                        color=[0, 0, 0],\n",
        "                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n",
        "            cv2.putText(im_copy, text=text,\n",
        "                        org=(x1 + 5, y1 + 15),\n",
        "                        thickness=1,\n",
        "                        fontScale=1,\n",
        "                        color=[0, 0, 0],\n",
        "                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n",
        "        cv2.addWeighted(im_copy, 0.7, im, 0.3, 0, im)\n",
        "        cv2.imwrite('samples/output_frcnn_{}.jpg'.format(sample_count), im)\n"
      ],
      "metadata": {
        "id": "1FdjwVQT-8vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infer(faster_rcnn_model, train_dataset)"
      ],
      "metadata": {
        "id": "O5OPGdqpha-z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}