{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02DLWtyWbH7g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import yaml\n",
        "import random\n",
        "import cv2\n",
        "import csv\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ARCHITECTURE + METHODS**"
      ],
      "metadata": {
        "id": "Wo18m2JY3zCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATASET DEFINITION**"
      ],
      "metadata": {
        "id": "VJDNWpzh7cpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Define Albumentations augmentation pipeline\n",
        "albumentations_transform = A.Compose([\n",
        "    A.RandomRotate90(p=0.5),                   # Random 90-degree rotations\n",
        "    A.HorizontalFlip(p=0.5),                  # Horizontal flipping\n",
        "    A.VerticalFlip(p=0.5),                    # Vertical flipping\n",
        "    A.GaussNoise(var_limit=(10, 50), p=0.5),  # Add random Gaussian noise\n",
        "    A.Normalize(mean=(0.5,), std=(0.5,)),     # Normalize grayscale values\n",
        "    ToTensorV2()                              # Convert to PyTorch tensor\n",
        "])"
      ],
      "metadata": {
        "id": "RUhRikl7LnrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5570b027-827f-4904-c605-448bde5b6f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "class IRDataset(Dataset):\n",
        "    def __init__(self, im_dir, ann_dir, transform=None):\n",
        "        self.im_dir = im_dir\n",
        "        self.ann_dir = ann_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Define the classes\n",
        "        classes = ['person', 'vehicle']\n",
        "        classes = sorted(classes)\n",
        "        classes = ['background'] + classes  # Background is index 0\n",
        "\n",
        "        self.label2idx = {classes[idx]: idx for idx in range(len(classes))}\n",
        "        self.idx2label = {idx: classes[idx] for idx in range(len(classes))}\n",
        "\n",
        "        # Load image information\n",
        "        self.images_info = get_image_info(im_dir, ann_dir, self.label2idx, classes)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_info)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        im_info = self.images_info[index]\n",
        "        im = Image.open(im_info['filename']).convert('RGB')\n",
        "        im_np = np.array(im)\n",
        "\n",
        "        im_tensor = torchvision.transforms.ToTensor()(im)\n",
        "\n",
        "        if self.transform:\n",
        "           augmented = self.transform(image=im_np)\n",
        "           im_tensor = augmented['image']\n",
        "        else:\n",
        "           im_tensor = torchvision.transforms.ToTensor()(im_np)\n",
        "\n",
        "\n",
        "        targets = {}\n",
        "        targets['bboxes'] = torch.as_tensor([detection['bbox'] for detection in im_info['detections']])\n",
        "        targets['labels'] = torch.as_tensor([detection['label'] for detection in im_info['detections']])\n",
        "\n",
        "        return im_tensor, targets, im_info['filename']\n"
      ],
      "metadata": {
        "id": "Fc0cEd1VLgHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_info(image_directory, annotation_directory, label2idx, classes):\n",
        "\n",
        "    im_infos = []\n",
        "\n",
        "    # Iterate over all files in the image directory\n",
        "    for filename in os.listdir(image_directory):\n",
        "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "            img_id = filename.split('.')[0]\n",
        "            img_path = os.path.join(image_directory, filename)\n",
        "\n",
        "            # Read image to get dimensions\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is None:  # Skip if the image cannot be read\n",
        "                continue\n",
        "            height, width, _ = image.shape\n",
        "\n",
        "            # Initialize detections list\n",
        "            detections = []\n",
        "\n",
        "            # Read corresponding annotation file\n",
        "            annotation_file = os.path.join(annotation_directory, f\"{img_id}.txt\")\n",
        "            if os.path.exists(annotation_file):\n",
        "                with open(annotation_file, 'r') as file:\n",
        "                    for line in file:\n",
        "                        parts = line.strip().split()\n",
        "                        if len(parts) == 5:\n",
        "                            class_id = int(parts[0])  # Original class ID from annotation\n",
        "\n",
        "                            # Map class ID to label index\n",
        "                            if class_id in range(len(label2idx) - 1):  # Check if class_id is valid\n",
        "                                mapped_label = label2idx[classes[class_id + 1]]  # Adjust index for 'background'\n",
        "\n",
        "                                x_center = float(parts[1]) * width\n",
        "                                y_center = float(parts[2]) * height\n",
        "                                box_width = float(parts[3]) * width\n",
        "                                box_height = float(parts[4]) * height\n",
        "\n",
        "                                # Calculate bounding box coordinates\n",
        "                                x_min = int(x_center - box_width / 2)\n",
        "                                y_min = int(y_center - box_height / 2)\n",
        "                                x_max = int(x_center + box_width / 2)\n",
        "                                y_max = int(y_center + box_height / 2)\n",
        "\n",
        "                                # Add detection with mapped label\n",
        "                                detections.append({\n",
        "                                    'label': mapped_label,\n",
        "                                    'bbox': [x_min, y_min, x_max, y_max]\n",
        "                                })\n",
        "\n",
        "            # Skip if no detections are found\n",
        "            if not detections:\n",
        "                continue\n",
        "\n",
        "            # Append image info to the list\n",
        "            im_info = {\n",
        "                'img_id': img_id,\n",
        "                'filename': img_path,\n",
        "                'width': width,\n",
        "                'height': height,\n",
        "                'detections': detections\n",
        "            }\n",
        "            im_infos.append(im_info)\n",
        "\n",
        "    return im_infos\n",
        "\n",
        "def collate_function(data):\n",
        "    return tuple(zip(*data))"
      ],
      "metadata": {
        "id": "brxUCO3NDVBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "def split_dataset(dataset, split_ratios=[0.7, 0.15, 0.15], seed=42):\n",
        "    \"\"\"\n",
        "    Splits a dataset into training, validation, and test subsets.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): The dataset to split.\n",
        "        split_ratios (list): A list of three floats for train, val, and test splits. Must sum to 1.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Datasets for training, validation, and test.\n",
        "    \"\"\"\n",
        "    assert sum(split_ratios) == 1.0, \"Split ratios must sum to 1.\"\n",
        "\n",
        "    # Calculate lengths for each split\n",
        "    dataset_size = len(dataset)\n",
        "    train_size = int(split_ratios[0] * dataset_size)\n",
        "    val_size = int(split_ratios[1] * dataset_size)\n",
        "    test_size = dataset_size - train_size - val_size  # Ensure no rounding errors\n",
        "\n",
        "    # Fix the random seed for reproducibility\n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "\n",
        "    # Perform the split\n",
        "    train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size], generator=generator)\n",
        "\n",
        "    return train_set, val_set, test_set"
      ],
      "metadata": {
        "id": "05mpkRuLNXSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directories for images and annotations\n",
        "im_dir = '/content/drive/MyDrive/Colab Notebooks/ir_images/ir_images'\n",
        "label_dir = '/content/drive/MyDrive/Colab Notebooks/ir_labels/ir_labels'"
      ],
      "metadata": {
        "id": "LIkszZydMvQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_info = get_image_info(im_dir, label_dir, {'background': 0, 'person': 1, 'vehicle': 2}, ['background', 'person', 'vehicle'])\n",
        "for info in image_info[:2]:  # Print the first 2 entries\n",
        "    print(info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdLUdN01rw3V",
        "outputId": "b4ffa199-b06f-41c8-9789-a4ea464f0b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'img_id': '20240821_122016_IR_H264_frame_004721', 'filename': '/content/drive/MyDrive/Colab Notebooks/ir_images/20240821_122016_IR_H264_frame_004721.jpg', 'width': 640, 'height': 512, 'detections': [{'label': 1, 'bbox': [233, 424, 260, 491]}, {'label': 1, 'bbox': [410, 417, 443, 493]}, {'label': 1, 'bbox': [486, 415, 514, 492]}, {'label': 1, 'bbox': [604, 412, 626, 489]}, {'label': 1, 'bbox': [625, 416, 640, 489]}, {'label': 1, 'bbox': [616, 414, 632, 489]}]}\n",
            "{'img_id': '20240821_122016_IR_H264_frame_005250', 'filename': '/content/drive/MyDrive/Colab Notebooks/ir_images/20240821_122016_IR_H264_frame_005250.jpg', 'width': 640, 'height': 512, 'detections': [{'label': 1, 'bbox': [362, 1, 528, 512]}, {'label': 1, 'bbox': [153, 417, 175, 481]}, {'label': 1, 'bbox': [51, 421, 70, 473]}, {'label': 1, 'bbox': [8, 417, 24, 468]}, {'label': 1, 'bbox': [28, 419, 42, 474]}, {'label': 1, 'bbox': [36, 426, 48, 475]}, {'label': 1, 'bbox': [0, 421, 14, 474]}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets for training, validation, and testing\n",
        "dataset = IRDataset(im_dir=im_dir, ann_dir=label_dir)\n",
        "len(dataset)"
      ],
      "metadata": {
        "id": "ZXbhZpFOMngn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL INITIALIZATION**"
      ],
      "metadata": {
        "id": "9cT7SZ3o7e6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Hyperparameters\n",
        "    Edit the values as necessary here '''\n",
        "\n",
        "rpn_pre_nms_top_n = 2000            # Number of proposals before NMS (filtering proposals)\n",
        "rpn_post_nms_top_n = 1000           # Number of proposals after NMS (filtering proposals)\n",
        "rpn_nms_thresh = 0.7                # IoU threshold for NMS\n",
        "rpn_fg_iou_thresh = 0.7             # Minimum IoU for a positive anchor\n",
        "rpn_bg_iou_thresh = 0.3             # Maximum IoU for a negative anchor\n",
        "rpn_batch_size_per_image = 256      # Number of anchors sampled per image during training\n",
        "\n",
        "box_detections_per_img = 9          # Maximum number of detections per image\n",
        "box_score_thresh = 0.05             # Min. score for a detection to be considered valid\n",
        "box_nms_thresh = 0.5                # IoU threshold for NMS on detection results\n",
        "box_fg_iou_thresh = 0.5             # Minimum IoU for a positive RoI during training\n",
        "box_bg_iou_thresh = 0.5             # Maximum IoU for a negative RoI during training\n",
        "\n",
        "batch_size = 4\n",
        "lr = 0.003\n",
        "num_epochs = 100\n",
        "momentum = 0.9\n",
        "weight_decay = 0\n",
        "acc_steps = 4                       # Can leave as 1\n",
        "\n",
        "loss_weights = [1, 1, 1, 1]\n",
        "\n",
        "split_ratios = [0.7, 0.15, 0.15]\n",
        "train_dataset, val_dataset, test_dataset = split_dataset(dataset, split_ratios)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=collate_function)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=collate_function)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=collate_function)"
      ],
      "metadata": {
        "id": "QCsHdjgvNY9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True,\n",
        "                                                             min_size=512,\n",
        "                                                             max_size=640,\n",
        "                                                             rpn_pre_nms_top_n=rpn_pre_nms_top_n,\n",
        "                                                             rpn_post_nms_top_n=rpn_post_nms_top_n,\n",
        "                                                             rpn_nms_thresh=rpn_nms_thresh,\n",
        "                                                             rpn_fg_iou_thresh=rpn_fg_iou_thresh,\n",
        "                                                             rpn_bg_iou_thresh=rpn_bg_iou_thresh,\n",
        "                                                             rpn_batch_size_per_image=rpn_batch_size_per_image,\n",
        "                                                             box_detections_per_img=box_detections_per_img,\n",
        "                                                             box_nms_thresh=box_nms_thresh,\n",
        "                                                             box_score_thresh=box_score_thresh,\n",
        "                                                             box_fg_iou_thresh=box_fg_iou_thresh,\n",
        "                                                             box_bg_iou_thresh=box_bg_iou_thresh,\n",
        ")\n",
        "\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(\n",
        "            model.roi_heads.box_predictor.cls_score.in_features,\n",
        "            num_classes=3)\n",
        "\n",
        "optimizer = torch.optim.SGD(lr=lr, momentum=momentum, weight_decay=weight_decay,\n",
        "                            params=filter(lambda p: p.requires_grad, model.parameters()))"
      ],
      "metadata": {
        "id": "ksgUZvyo7sHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAINING ROUTINE**"
      ],
      "metadata": {
        "id": "iDAYXVdo7jnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, dataset, train_loader, val_loader, optimizer, loss_weights, num_epochs, acc_steps, device)"
      ],
      "metadata": {
        "id": "1ZRkP6OWH6Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset))\n",
        "print(len(train_dataset))\n",
        "print(len(train_loader))\n",
        "print(len(val_dataset))\n",
        "print(len(val_loader))\n",
        "print(len(test_dataset))\n",
        "print(len(test_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZo5Sr0CCZvM",
        "outputId": "d4a21dad-e33e-4a3b-b1e5-e4320207f920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1108\n",
            "775\n",
            "194\n",
            "166\n",
            "42\n",
            "167\n",
            "42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_name(name, batch_size, learning_rate, epoch):\n",
        "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object containing the hyperparameters\n",
        "    Returns:\n",
        "        path: A string with the hyperparameter name and value concatenated\n",
        "    \"\"\"\n",
        "    path = \"model_{0}_lr{1}_epoch{2}\".format(name,\n",
        "                                                   learning_rate,\n",
        "                                                   epoch)\n",
        "    return path"
      ],
      "metadata": {
        "id": "YJOEnChhXPXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataset, train_loader, val_loader, optimizer, loss_weights, num_epochs, acc_steps=1, device='cpu'):\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    # Lists to store losses\n",
        "    train_rpn_class_loss_history = []\n",
        "    train_rpn_reg_loss_history = []\n",
        "    train_detector_class_loss_history = []\n",
        "    train_detector_reg_loss_history = []\n",
        "    train_loss_history = []\n",
        "\n",
        "    val_rpn_class_loss_history = []\n",
        "    val_rpn_reg_loss_history = []\n",
        "    val_detector_class_loss_history = []\n",
        "    val_detector_reg_loss_history = []\n",
        "    val_loss_history = []\n",
        "\n",
        "    # Lists for mAPs\n",
        "    mean_ap_history = []\n",
        "    all_aps_history = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_rpn_class_losses = []\n",
        "        train_rpn_reg_losses = []\n",
        "        train_detector_class_losses = []\n",
        "        train_detector_reg_losses = []\n",
        "\n",
        "        val_rpn_class_losses = []\n",
        "        val_rpn_reg_losses = []\n",
        "        val_detector_class_losses = []\n",
        "        val_detector_reg_losses = []\n",
        "\n",
        "        gts = []\n",
        "        preds = []\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Training Loop\n",
        "        for ims, targets, _ in tqdm(train_loader, desc='Training'):\n",
        "            optimizer.zero_grad()\n",
        "            for target in targets:\n",
        "              #raise ValueError(targets)\n",
        "              target['boxes'] = target['bboxes'].float().to(device)\n",
        "              del target['bboxes']\n",
        "              target['labels'] = target['labels'].long().to(device)\n",
        "            images = [im.float().to(device) for im in ims]\n",
        "            batch_losses = model(images, targets)\n",
        "\n",
        "            loss = loss_weights[0]*batch_losses['loss_classifier']\n",
        "            loss += loss_weights[1]*batch_losses['loss_box_reg']\n",
        "            loss += loss_weights[2]*batch_losses['loss_objectness']\n",
        "            loss += loss_weights[3]*batch_losses['loss_rpn_box_reg']\n",
        "\n",
        "            train_rpn_class_losses.append(batch_losses['loss_objectness'].item())\n",
        "            train_rpn_reg_losses.append(batch_losses['loss_rpn_box_reg'].item())\n",
        "            train_detector_class_losses.append(batch_losses['loss_classifier'].item())\n",
        "            train_detector_reg_losses.append(batch_losses['loss_box_reg'].item())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            break\n",
        "\n",
        "        # Evaluate mAP\n",
        "        mean_ap, all_aps, _, _, _, _, _, _ = evaluate_map(model, dataset, val_loader, device)\n",
        "        model.train()\n",
        "\n",
        "        print('Mean Average Precision : {:.4f}'.format(mean_ap))\n",
        "\n",
        "        for param in model.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "        # Validation Loop\n",
        "        with torch.no_grad():\n",
        "            for ims, targets, _ in tqdm(val_loader):\n",
        "                for target in targets:\n",
        "                  #raise ValueError(targets)\n",
        "                  target['boxes'] = target['bboxes'].float().to(device)\n",
        "                  del target['bboxes']\n",
        "                  target['labels'] = target['labels'].long().to(device)\n",
        "                images = [im.float().to(device) for im in ims]\n",
        "                batch_losses = model(images, targets)\n",
        "\n",
        "                val_rpn_class_losses.append(batch_losses['loss_objectness'].item())\n",
        "                val_rpn_reg_losses.append(batch_losses['loss_rpn_box_reg'].item())\n",
        "                val_detector_class_losses.append(batch_losses['loss_classifier'].item())\n",
        "                val_detector_reg_losses.append(batch_losses['loss_box_reg'].item())\n",
        "\n",
        "        for param in model.parameters():\n",
        "          param.requires_grad = True\n",
        "\n",
        "        print('Finished epoch {}'.format(epoch))\n",
        "\n",
        "        # Calculate average losses for the epoch\n",
        "        avg_train_rpn_classification_loss = np.mean(train_rpn_class_losses)\n",
        "        avg_train_rpn_regression_loss = np.mean(train_rpn_reg_losses)\n",
        "        avg_train_detector_classification_loss = np.mean(train_detector_class_losses)\n",
        "        avg_train_detector_regression_loss = np.mean(train_detector_reg_losses)\n",
        "\n",
        "        avg_val_rpn_classification_loss = np.mean(val_rpn_class_losses)\n",
        "        avg_val_rpn_regression_loss = np.mean(val_rpn_reg_losses)\n",
        "        avg_val_detector_classification_loss = np.mean(val_detector_class_losses)\n",
        "        avg_val_detector_regression_loss = np.mean(val_detector_reg_losses)\n",
        "\n",
        "        # Store the average losses\n",
        "        train_rpn_class_loss_history.append(avg_train_rpn_classification_loss)\n",
        "        train_rpn_reg_loss_history.append(avg_train_rpn_regression_loss)\n",
        "        train_detector_class_loss_history.append(avg_train_detector_classification_loss)\n",
        "        train_detector_reg_loss_history.append(avg_train_detector_regression_loss)\n",
        "\n",
        "        val_rpn_class_loss_history.append(avg_val_rpn_classification_loss)\n",
        "        val_rpn_reg_loss_history.append(avg_val_rpn_regression_loss)\n",
        "        val_detector_class_loss_history.append(avg_val_detector_classification_loss)\n",
        "        val_detector_reg_loss_history.append(avg_val_detector_regression_loss)\n",
        "\n",
        "        # Store mAP\n",
        "        mean_ap_history.append(mean_ap)\n",
        "        all_aps_history.append(all_aps)\n",
        "\n",
        "        # Save\n",
        "        model_path = get_model_name('frcnn', batch_size, lr, epoch)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "        # Prepare the loss output string\n",
        "        loss_output = ''\n",
        "        loss_output += 'RPN Classification Loss (Train): {:.4f}'.format(avg_train_rpn_classification_loss)\n",
        "        loss_output += ' | RPN Localization Loss (Train): {:.4f}'.format(avg_train_rpn_regression_loss)\n",
        "        loss_output += ' | FRCNN Classification Loss (Train): {:.4f}'.format(avg_train_detector_classification_loss)\n",
        "        loss_output += ' | FRCNN Localization Loss (Train): {:.4f}'.format(avg_train_detector_regression_loss)\n",
        "\n",
        "        # Add validation losses to the printout\n",
        "        loss_output += ' | RPN Classification Loss (Val): {:.4f}'.format(avg_val_rpn_classification_loss)\n",
        "        loss_output += ' | RPN Localization Loss (Val): {:.4f}'.format(avg_val_rpn_regression_loss)\n",
        "        loss_output += ' | FRCNN Classification Loss (Val): {:.4f}'.format(avg_val_detector_classification_loss)\n",
        "        loss_output += ' | FRCNN Localization Loss (Val): {:.4f}'.format(avg_val_detector_regression_loss)\n",
        "\n",
        "        loss_output += ' | Mean Average Precision: {:.4f}'.format(mean_ap)\n",
        "\n",
        "        # Print out the losses and mAP\n",
        "        print(loss_output)\n",
        "\n",
        "\n",
        "    save_results(model_path, train_rpn_class_loss_history, train_rpn_reg_loss_history, train_detector_class_loss_history,\n",
        "             train_detector_reg_loss_history, val_rpn_class_loss_history, val_rpn_reg_loss_history, val_detector_class_loss_history,\n",
        "             val_detector_reg_loss_history, mean_ap_history, all_aps_history, num_epochs)\n",
        "    print('Done Training...')"
      ],
      "metadata": {
        "id": "YyxwVLP--H5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_results(model_path, train_rpn_class_loss_history, train_rpn_reg_loss_history, train_detector_class_loss_history,\n",
        "                 train_detector_reg_loss_history, val_rpn_class_loss_history, val_rpn_reg_loss_history, val_detector_class_loss_history,\n",
        "                 val_detector_reg_loss_history, mean_ap_history, all_aps_history, num_epochs):\n",
        "\n",
        "    # Extract the base directory and filename from model_path\n",
        "    base_dir, model_name = os.path.split(model_path)\n",
        "    results_dir = os.path.join(base_dir, f\"{model_name}_results\")\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    # Define the file paths for each CSV\n",
        "    train_rpn_class_loss_path = os.path.join(results_dir, \"train_rpn_class_loss.csv\")\n",
        "    train_rpn_reg_loss_path = os.path.join(results_dir, \"train_rpn_reg_loss.csv\")\n",
        "    train_detector_class_loss_path = os.path.join(results_dir, \"train_detector_class_loss.csv\")\n",
        "    train_detector_reg_loss_path = os.path.join(results_dir, \"train_detector_reg_loss.csv\")\n",
        "\n",
        "    val_rpn_class_loss_path = os.path.join(results_dir, \"val_rpn_class_loss.csv\")\n",
        "    val_rpn_reg_loss_path = os.path.join(results_dir, \"val_rpn_reg_loss.csv\")\n",
        "    val_detector_class_loss_path = os.path.join(results_dir, \"val_detector_class_loss.csv\")\n",
        "    val_detector_reg_loss_path = os.path.join(results_dir, \"val_detector_reg_loss.csv\")\n",
        "\n",
        "    mean_ap_path = os.path.join(results_dir, \"mean_ap.csv\")\n",
        "    all_aps_path = os.path.join(results_dir, \"all_aps.csv\")  # Update for CSV\n",
        "\n",
        "    # Save the results into CSV files\n",
        "    np.savetxt(train_rpn_class_loss_path, train_rpn_class_loss_history)\n",
        "    np.savetxt(train_rpn_reg_loss_path, train_rpn_reg_loss_history)\n",
        "    np.savetxt(train_detector_class_loss_path, train_detector_class_loss_history)\n",
        "    np.savetxt(train_detector_reg_loss_path, train_detector_reg_loss_history)\n",
        "\n",
        "    np.savetxt(val_rpn_class_loss_path, val_rpn_class_loss_history)\n",
        "    np.savetxt(val_rpn_reg_loss_path, val_rpn_reg_loss_history)\n",
        "    np.savetxt(val_detector_class_loss_path, val_detector_class_loss_history)\n",
        "    np.savetxt(val_detector_reg_loss_path, val_detector_reg_loss_history)\n",
        "\n",
        "    np.savetxt(mean_ap_path, mean_ap_history)\n",
        "\n",
        "    # Save all_aps_history as CSV\n",
        "    with open(all_aps_path, mode='w', newline='') as csvfile:\n",
        "        # Extract unique keys from all_aps_history (excluding 'background')\n",
        "        fieldnames = {key for aps in all_aps_history for key in aps.keys() if key != 'background'}\n",
        "        fieldnames = sorted(fieldnames)  # Sort to maintain consistent column order\n",
        "\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()  # Write the header row\n",
        "\n",
        "        for aps in all_aps_history:\n",
        "            # Exclude 'background' and write the filtered dictionary\n",
        "            filtered_aps = {k: aps[k] for k in aps if k != 'background'}\n",
        "            writer.writerow(filtered_aps)\n",
        "\n",
        "    print(f'Results saved in folder: {results_dir}')"
      ],
      "metadata": {
        "id": "Ft1WmEc4vZbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_curves(path):\n",
        "      \"\"\" Plots the training curve for a model run, given the csv files\n",
        "    containing the train/validation error/loss.\n",
        "\n",
        "    Args:\n",
        "        path: The base path of the csv files produced during training\n",
        "    \"\"\"\n",
        "\n",
        "      train_rpn_class_loss = np.loadtxt(\"{}/train_rpn_class_loss.csv\".format(path))\n",
        "      train_rpn_reg_loss = np.loadtxt(\"{}/train_rpn_reg_loss.csv\".format(path))\n",
        "      train_detector_class_loss = np.loadtxt(\"{}/train_detector_class_loss.csv\".format(path))\n",
        "      train_detector_reg_loss = np.loadtxt(\"{}/train_detector_reg_loss.csv\".format(path))\n",
        "\n",
        "      val_rpn_class_loss = np.loadtxt(\"{}/val_rpn_class_loss.csv\".format(path))\n",
        "      val_rpn_reg_loss = np.loadtxt(\"{}/val_rpn_reg_loss.csv\".format(path))\n",
        "      val_detector_class_loss = np.loadtxt(\"{}/val_detector_class_loss.csv\".format(path))\n",
        "      val_detector_reg_loss = np.loadtxt(\"{}/val_detector_reg_loss.csv\".format(path))\n",
        "\n",
        "      mean_ap = np.loadtxt(\"{}/mean_ap.csv\".format(path))\n",
        "\n",
        "      train_loss = train_rpn_class_loss + train_rpn_reg_loss + train_detector_class_loss + train_detector_reg_loss\n",
        "      val_loss = val_rpn_class_loss + val_rpn_reg_loss + val_detector_class_loss + val_detector_reg_loss\n",
        "\n",
        "      n = len(train_loss)\n",
        "\n",
        "      plt.title(\"RPN Class Loss\")\n",
        "      plt.plot(range(1,n+1), train_rpn_class_loss, label=\"Train\")\n",
        "      plt.plot(range(1,n+1), val_rpn_class_loss, label=\"Val\")\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.legend(loc='best')\n",
        "      plot_title = os.path.join(path, \"rpn_class\")\n",
        "      plt.savefig(plot_title)\n",
        "      plt.show()\n",
        "\n",
        "      plt.title(\"RPN Regression Loss\")\n",
        "      plt.plot(range(1,n+1), train_rpn_reg_loss, label=\"Train\")\n",
        "      plt.plot(range(1,n+1), val_rpn_reg_loss, label=\"Val\")\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.legend(loc='best')\n",
        "      plot_title = os.path.join(path, \"rpn_reg\")\n",
        "      plt.savefig(plot_title)\n",
        "      plt.show()\n",
        "\n",
        "      plt.title(\"Detector Class Loss\")\n",
        "      plt.plot(range(1,n+1), train_detector_class_loss, label=\"Train\")\n",
        "      plt.plot(range(1,n+1), val_detector_class_loss, label=\"Val\")\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.legend(loc='best')\n",
        "      plot_title = os.path.join(path, \"detector_class\")\n",
        "      plt.savefig(plot_title)\n",
        "      plt.show()\n",
        "\n",
        "      plt.title(\"Detector Regression Loss\")\n",
        "      plt.plot(range(1,n+1), train_detector_reg_loss, label=\"Train\")\n",
        "      plt.plot(range(1,n+1), val_detector_reg_loss, label=\"Val\")\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.legend(loc='best')\n",
        "      plot_title = os.path.join(path, \"detector_reg\")\n",
        "      plt.savefig(plot_title)\n",
        "      plt.show()\n",
        "\n",
        "      plt.title(\"Total Loss\")\n",
        "      plt.plot(range(1,n+1), train_loss, label=\"Train\")\n",
        "      plt.plot(range(1,n+1), val_loss, label=\"Val\")\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.legend(loc='best')\n",
        "      plot_title = os.path.join(path, \"total_loss\")\n",
        "      plt.savefig(plot_title)\n",
        "      plt.show()\n",
        "\n",
        "      plt.title(\"Mean Average Precision\")\n",
        "      plt.plot(range(1,n+1), mean_ap)\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"mAP\")\n",
        "      plot_title = os.path.join(path, \"mAP\")\n",
        "      plt.savefig(plot_title)\n",
        "      plt.show()\n"
      ],
      "metadata": {
        "id": "wFZk-MWGa7jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_map(model, dataset, val_loader, device, iou_threshold=0.5):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    gts = []\n",
        "    preds = []\n",
        "    for im, target, _ in tqdm(val_loader, desc='Evaluating mAP'):\n",
        "        #im_name = fname\n",
        "        #raise ValueError(im[0])\n",
        "        im = im[0]\n",
        "        im = im.float().to(device)\n",
        "        im = im.unsqueeze(0)\n",
        "\n",
        "        target = target[0]\n",
        "        #raise ValueError(target)\n",
        "        target_boxes = target['bboxes'].float().to(device)\n",
        "        target_labels = target['labels'].long().to(device)\n",
        "        output = model(im, None)[0]\n",
        "\n",
        "        boxes = output['boxes']\n",
        "        labels = output['labels']\n",
        "        scores = output['scores']\n",
        "\n",
        "        pred_boxes = {}\n",
        "        gt_boxes = {}\n",
        "        for label_name in dataset.label2idx:\n",
        "            pred_boxes[label_name] = []\n",
        "            gt_boxes[label_name] = []\n",
        "\n",
        "        for idx, box in enumerate(boxes):\n",
        "            x1, y1, x2, y2 = box.detach().cpu().numpy()\n",
        "            label = labels[idx].detach().cpu().item()\n",
        "            score = scores[idx].detach().cpu().item()\n",
        "            label_name = dataset.idx2label[label]\n",
        "            pred_boxes[label_name].append([x1, y1, x2, y2, score])\n",
        "\n",
        "        for idx, box in enumerate(target_boxes):\n",
        "            x1, y1, x2, y2 = box.detach().cpu().numpy()\n",
        "            label = target_labels[idx].detach().cpu().item()\n",
        "            label_name = dataset.idx2label[label]\n",
        "            gt_boxes[label_name].append([x1, y1, x2, y2])\n",
        "\n",
        "        gts.append(gt_boxes)\n",
        "        preds.append(pred_boxes)\n",
        "\n",
        "    mean_ap, all_aps, mean_recall, mean_precision, mean_f1, all_recalls, all_precisions, all_f1_scores = compute_map(preds, gts, iou_threshold)\n",
        "    return mean_ap, all_aps, mean_recall, mean_precision, mean_f1, all_recalls, all_precisions, all_f1_scores"
      ],
      "metadata": {
        "id": "pEqk5wX_-oQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_iou(det, gt):\n",
        "    det_x1, det_y1, det_x2, det_y2 = det\n",
        "    gt_x1, gt_y1, gt_x2, gt_y2 = gt\n",
        "\n",
        "    x_left = max(det_x1, gt_x1)\n",
        "    y_top = max(det_y1, gt_y1)\n",
        "    x_right = min(det_x2, gt_x2)\n",
        "    y_bottom = min(det_y2, gt_y2)\n",
        "\n",
        "    if x_right < x_left or y_bottom < y_top:\n",
        "        return 0.0\n",
        "\n",
        "    area_intersection = (x_right - x_left) * (y_bottom - y_top)\n",
        "    det_area = (det_x2 - det_x1) * (det_y2 - det_y1)\n",
        "    gt_area = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)\n",
        "    area_union = float(det_area + gt_area - area_intersection + 1E-6)\n",
        "    iou = area_intersection / area_union\n",
        "    return iou"
      ],
      "metadata": {
        "id": "PGCC3kpK_c--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_map(det_boxes, gt_boxes, iou_threshold=0.5):\n",
        "    gt_labels = {cls_key for im_gt in gt_boxes for cls_key in im_gt.keys()}\n",
        "    gt_labels = sorted(gt_labels)\n",
        "    all_aps = {}\n",
        "    all_recalls = {}\n",
        "    all_precisions = {}\n",
        "    all_f1_scores = {}\n",
        "\n",
        "    aps = []  # Average Precisions for all classes\n",
        "    recalls_per_class = []  # Average Recalls per class\n",
        "    precisions_per_class = []  # Average Precisions per class\n",
        "    f1_scores_per_class = []  # F1-scores per class\n",
        "\n",
        "    for idx, label in enumerate(gt_labels):\n",
        "        cls_dets = [\n",
        "            [im_idx, im_dets_label] for im_idx, im_dets in enumerate(det_boxes)\n",
        "            if label in im_dets for im_dets_label in im_dets[label]\n",
        "        ]\n",
        "\n",
        "        cls_dets = sorted(cls_dets, key=lambda k: -k[1][-1])\n",
        "        gt_matched = [[False for _ in im_gts[label]] for im_gts in gt_boxes]\n",
        "        num_gts = sum([len(im_gts[label]) for im_gts in gt_boxes])\n",
        "        tp = [0] * len(cls_dets)\n",
        "        fp = [0] * len(cls_dets)\n",
        "\n",
        "        for det_idx, (im_idx, det_pred) in enumerate(cls_dets):\n",
        "            im_gts = gt_boxes[im_idx][label]\n",
        "            max_iou_found = -1\n",
        "            max_iou_gt_idx = -1\n",
        "\n",
        "            for gt_box_idx, gt_box in enumerate(im_gts):\n",
        "                gt_box_iou = get_iou(det_pred[:-1], gt_box)\n",
        "                if gt_box_iou > max_iou_found:\n",
        "                    max_iou_found = gt_box_iou\n",
        "                    max_iou_gt_idx = gt_box_idx\n",
        "\n",
        "            if max_iou_found < iou_threshold or gt_matched[im_idx][max_iou_gt_idx]:\n",
        "                fp[det_idx] = 1\n",
        "            else:\n",
        "                tp[det_idx] = 1\n",
        "                gt_matched[im_idx][max_iou_gt_idx] = True\n",
        "\n",
        "        tp = np.cumsum(tp)\n",
        "        fp = np.cumsum(fp)\n",
        "\n",
        "        eps = np.finfo(np.float32).eps\n",
        "        recalls = tp / np.maximum(num_gts, eps)\n",
        "        precisions = tp / np.maximum((tp + fp), eps)\n",
        "\n",
        "        recalls = np.concatenate(([0.0], recalls, [1.0]))\n",
        "        precisions = np.concatenate(([0.0], precisions, [0.0]))\n",
        "\n",
        "        for i in range(precisions.size - 1, 0, -1):\n",
        "            precisions[i - 1] = np.maximum(precisions[i - 1], precisions[i])\n",
        "        i = np.where(recalls[1:] != recalls[:-1])[0]\n",
        "        ap = np.sum((recalls[i + 1] - recalls[i]) * precisions[i + 1])\n",
        "\n",
        "        if num_gts > 0:\n",
        "            aps.append(ap)\n",
        "            all_aps[label] = ap\n",
        "            avg_recall = recalls[-2]  # Recall at the last predicted TP\n",
        "            avg_precision = precisions[np.argmax(recalls > 0)]  # Precision at max recall > 0\n",
        "\n",
        "            # F1-Score Calculation\n",
        "            if avg_precision + avg_recall > 0:\n",
        "                f1_score = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall)\n",
        "            else:\n",
        "                f1_score = 0\n",
        "\n",
        "            # Store metrics\n",
        "            all_recalls[label] = avg_recall\n",
        "            all_precisions[label] = avg_precision\n",
        "            all_f1_scores[label] = f1_score\n",
        "\n",
        "            recalls_per_class.append(avg_recall)\n",
        "            precisions_per_class.append(avg_precision)\n",
        "            f1_scores_per_class.append(f1_score)\n",
        "        else:\n",
        "            all_aps[label] = np.nan\n",
        "            all_recalls[label] = np.nan\n",
        "            all_precisions[label] = np.nan\n",
        "            all_f1_scores[label] = np.nan\n",
        "\n",
        "    mean_ap = sum(aps) / len(aps) if aps else 0\n",
        "    mean_recall = sum(recalls_per_class) / len(recalls_per_class) if recalls_per_class else 0\n",
        "    mean_precision = sum(precisions_per_class) / len(precisions_per_class) if precisions_per_class else 0\n",
        "    mean_f1 = sum(f1_scores_per_class) / len(f1_scores_per_class) if f1_scores_per_class else 0\n",
        "\n",
        "    return mean_ap, all_aps, mean_recall, mean_precision, mean_f1, all_recalls, all_precisions, all_f1_scores"
      ],
      "metadata": {
        "id": "kiioTgJsYG4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pascal_map(model, dataset, loader, device):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    maps = []\n",
        "\n",
        "    for i in np.arange(0.5, 0.95, 0.05):\n",
        "        mean_ap, all_aps, _, _, _, _, _, _ = evaluate_map(model, dataset, loader, device, iou_threshold=i)\n",
        "        maps.append(mean_ap)\n",
        "\n",
        "    return np.mean(maps)"
      ],
      "metadata": {
        "id": "vDGkUSXZET_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INFERENCE**"
      ],
      "metadata": {
        "id": "rY-dtxFS7lxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(model, dataset, test_loader, output_dir, device, score_threshold):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.mkdir(output_dir)\n",
        "\n",
        "    num_preds = []\n",
        "\n",
        "    for sample_count in tqdm(range(9)):\n",
        "        random_idx = random.randint(0, len(dataset)-1)\n",
        "        im, target, fname = dataset[random_idx]\n",
        "        im = im.unsqueeze(0).float().to(device)\n",
        "\n",
        "        gt_im = cv2.imread(fname)\n",
        "        gt_im_copy = gt_im.copy()\n",
        "\n",
        "        num_preds = target['bboxes'].size(0)\n",
        "        # Saving images with ground truth boxes\n",
        "        for idx, box in enumerate(target['bboxes']):\n",
        "            x1, y1, x2, y2 = box.detach().cpu().numpy()\n",
        "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "\n",
        "            cv2.rectangle(gt_im, (x1, y1), (x2, y2), thickness=2, color=[0, 255, 0])\n",
        "            cv2.rectangle(gt_im_copy, (x1, y1), (x2, y2), thickness=2, color=[0, 255, 0])\n",
        "            text = dataset.idx2label[target['labels'][idx].detach().cpu().item()]\n",
        "            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, 1, 1)\n",
        "            text_w, text_h = text_size\n",
        "            cv2.rectangle(gt_im_copy, (x1, y1), (x1 + 10 + text_w, y1 + 10 + text_h), [255, 255, 255], -1)\n",
        "            cv2.putText(gt_im, text=dataset.idx2label[target['labels'][idx].detach().cpu().item()],\n",
        "                        org=(x1 + 5, y1 + 15),\n",
        "                        thickness=1,\n",
        "                        fontScale=1,\n",
        "                        color=[0, 0, 0],\n",
        "                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n",
        "            cv2.putText(gt_im_copy, text=text,\n",
        "                        org=(x1 + 5, y1 + 15),\n",
        "                        thickness=1,\n",
        "                        fontScale=1,\n",
        "                        color=[0, 0, 0],\n",
        "                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n",
        "        cv2.addWeighted(gt_im_copy, 0.7, gt_im, 0.3, 0, gt_im)\n",
        "        cv2.imwrite('{}/output_gt_{}.png'.format(output_dir, sample_count), gt_im)\n",
        "\n",
        "        # Getting predictions from trained model\n",
        "        output = model(im, None)[0]\n",
        "        boxes = output['boxes']\n",
        "        labels = output['labels']\n",
        "        scores = output['scores']\n",
        "\n",
        "        # Filter predictions based on score_threshold\n",
        "        valid_indices = scores >= score_threshold\n",
        "        boxes = boxes[valid_indices]\n",
        "        labels = labels[valid_indices]\n",
        "        scores = scores[valid_indices]\n",
        "\n",
        "        im = cv2.imread(fname)\n",
        "        im_copy = im.copy()\n",
        "\n",
        "        # Saving images with predicted boxes\n",
        "        for idx, box in enumerate(boxes):\n",
        "            x1, y1, x2, y2 = box.detach().cpu().numpy()\n",
        "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "            cv2.rectangle(im, (x1, y1), (x2, y2), thickness=2, color=[0, 0, 255])\n",
        "            cv2.rectangle(im_copy, (x1, y1), (x2, y2), thickness=2, color=[0, 0, 255])\n",
        "            text = '{} : {:.2f}'.format(dataset.idx2label[labels[idx].detach().cpu().item()],\n",
        "                                        scores[idx].detach().cpu().item())\n",
        "            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, 1, 1)\n",
        "            text_w, text_h = text_size\n",
        "            cv2.rectangle(im_copy, (x1, y1), (x1 + 10 + text_w, y1 + 10 + text_h), [255, 255, 255], -1)\n",
        "            cv2.putText(im, text=text,\n",
        "                        org=(x1 + 5, y1 + 15),\n",
        "                        thickness=1,\n",
        "                        fontScale=1,\n",
        "                        color=[0, 0, 0],\n",
        "                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n",
        "            cv2.putText(im_copy, text=text,\n",
        "                        org=(x1 + 5, y1 + 15),\n",
        "                        thickness=1,\n",
        "                        fontScale=1,\n",
        "                        color=[0, 0, 0],\n",
        "                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n",
        "        cv2.addWeighted(im_copy, 0.7, im, 0.3, 0, im)\n",
        "        cv2.imwrite('{}/output_{}.jpg'.format(output_dir, sample_count), im)"
      ],
      "metadata": {
        "id": "Wdi5OSw17sw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EVALUATION**"
      ],
      "metadata": {
        "id": "AvzsTizh7pWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_curves('/content/model_frcnn_lr0.003_epoch99_results')"
      ],
      "metadata": {
        "id": "mZUqId5R7tF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infer(model, dataset, val_loader, 'yes', device, 0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vip82d8SVXD",
        "outputId": "9640c796-4f48-4b19-f4d2-681be71f9722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:28<00:00,  3.15s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TESTING**"
      ],
      "metadata": {
        "id": "3ykeh1F4PRLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_map(model, dataset, test_loader, device, iou_threshold=0.9)"
      ],
      "metadata": {
        "id": "nCqgrLCiPN2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SAVE DATA**"
      ],
      "metadata": {
        "id": "RzWnx33ORn2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/model_frcnn_lr0.003_epoch99_results.zip /content/model_frcnn_lr0.003_epoch99_results"
      ],
      "metadata": {
        "id": "vef5pFG3Rsc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a131b7b7-a547-48ef-aae1-d80f454809ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/model_frcnn_lr0.003_epoch99_results/ (stored 0%)\n",
            "  adding: content/model_frcnn_lr0.003_epoch99_results/train_detector_reg_loss.csv (deflated 56%)\n",
            "  adding: content/model_frcnn_lr0.003_epoch99_results/detector_class.png (deflated 7%)\n",
            "  adding: content/model_frcnn_lr0.003_epoch99_results/mAP.png (deflated 8%)\n",
            "  adding: content/model_frcnn_lr0.003_epoch99_results/total_loss.png (deflated 4%)\n",
            "  adding: content/model_frcnn_lr0.003_epoch99_results/train_rpn_class_loss.csv (deflated 54%)\n",
            "  adding: content/model_frcnn_lr0.003_epoch99_results/rpn_class.png (deflated 6%)\n",
            "  adding: content/model_frcnn_lr0.003_epoch99_results/train_detector_class_loss.csv (deflated 54%)\n",
            "  adding: content/model_frcnn_lr0.003_epoch99_results/train_rpn_reg_loss.csv (deflated 54%)\n",
            "  adding: content/model_frcnn_lr0.003_epoch99_results/val_detector_reg_loss.csv (deflated 57%)\n",
            "  adding: content/model_frcnn_lr0.003_epoch99_results/rpn_reg.png (deflated 6%)\n",
            "  adding: content/model_frcnn_lr0.003_epoch99_results/val_detector_class_loss.csv (deflated 56%)\n",
            "  adding: content/model_frcnn_lr0.003_epoch99_results/val_rpn_class_loss.csv (deflated 56%)\n",
            "  adding: content/model_frcnn_lr0.003_epoch99_results/detector_reg.png (deflated 4%)\n",
            "  adding: content/model_frcnn_lr0.003_epoch99_results/val_rpn_reg_loss.csv (deflated 58%)\n",
            "  adding: content/model_frcnn_lr0.003_epoch99_results/all_aps.csv (deflated 62%)\n",
            "  adding: content/model_frcnn_lr0.003_epoch99_results/mean_ap.csv (deflated 56%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/model_frcnn_lr0.003_epoch99_results.zip\")"
      ],
      "metadata": {
        "id": "qvWgHybdRtQH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "854bec97-d37e-4bbb-a54e-0f6467c7bcb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_07f6ee4e-59ec-4466-8d84-77597c142ec1\", \"model_frcnn_lr0.003_epoch99_results.zip\", 254497)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLVIP DATASET**\n"
      ],
      "metadata": {
        "id": "7BryS1sYZ4q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET"
      ],
      "metadata": {
        "id": "UUwFywIcoloS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLVIPDataset(Dataset):\n",
        "    def __init__(self, im_dir, ann_dir):\n",
        "        self.im_dir = im_dir\n",
        "        self.ann_dir = ann_dir\n",
        "\n",
        "        # Define the classes\n",
        "        classes = ['person', 'vehicle']\n",
        "        classes = sorted(classes)\n",
        "        classes = ['background'] + classes  # Background is index 0\n",
        "\n",
        "        self.label2idx = {classes[idx]: idx for idx in range(len(classes))}\n",
        "        self.idx2label = {idx: classes[idx] for idx in range(len(classes))}\n",
        "\n",
        "        # Load image information\n",
        "        self.images_info = LLVIP_get_image_info(im_dir, ann_dir, self.label2idx, classes)\n",
        "\n",
        "        print(f\"Loaded {len(self.images_info)} images.\")\n",
        "        if self.images_info:\n",
        "            print(f\"First entry: {self.images_info[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_info)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        im_info = self.images_info[index]\n",
        "        im = Image.open(im_info['filename']).convert('RGB')\n",
        "\n",
        "        im_tensor = torchvision.transforms.ToTensor()(im)\n",
        "\n",
        "        targets = {}\n",
        "        targets['bboxes'] = torch.as_tensor([detection['bbox'] for detection in im_info['detections']])\n",
        "        targets['labels'] = torch.as_tensor([detection['label'] for detection in im_info['detections']])\n",
        "\n",
        "        return im_tensor, targets, im_info['filename']"
      ],
      "metadata": {
        "id": "BJ-iGlMiZ7hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LLVIP_get_image_info(image_directory, annotation_directory, label2idx, classes):\n",
        "    im_infos = []\n",
        "\n",
        "    # Iterate over all files in the annotation directory\n",
        "    for annotation_filename in os.listdir(annotation_directory):\n",
        "        if annotation_filename.endswith('.xml'):\n",
        "            # Parse the XML file\n",
        "            annotation_path = os.path.join(annotation_directory, annotation_filename)\n",
        "            tree = ET.parse(annotation_path)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            # Get the image filename\n",
        "            filename = root.find('filename').text\n",
        "            img_id = filename.split('.')[0]\n",
        "            img_path = os.path.join(image_directory, filename)\n",
        "\n",
        "            # Read the image to get dimensions\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is None:  # Skip if the image cannot be read\n",
        "                continue\n",
        "            height, width, _ = image.shape\n",
        "\n",
        "            # Initialize detections list\n",
        "            detections = []\n",
        "\n",
        "            # Parse each object in the XML file\n",
        "            for obj in root.findall('object'):\n",
        "                class_name = obj.find('name').text\n",
        "\n",
        "                # Map class name to label index\n",
        "                if class_name in label2idx:\n",
        "                    mapped_label = label2idx[class_name]\n",
        "\n",
        "                    # Get bounding box coordinates\n",
        "                    bndbox = obj.find('bndbox')\n",
        "                    x_min = int(bndbox.find('xmin').text)\n",
        "                    y_min = int(bndbox.find('ymin').text)\n",
        "                    x_max = int(bndbox.find('xmax').text)\n",
        "                    y_max = int(bndbox.find('ymax').text)\n",
        "\n",
        "                    # Add detection\n",
        "                    detections.append({\n",
        "                        'label': mapped_label,\n",
        "                        'bbox': [x_min, y_min, x_max, y_max]\n",
        "                    })\n",
        "\n",
        "            # Skip if no detections are found\n",
        "            if not detections:\n",
        "                continue\n",
        "\n",
        "            # Append image info to the list\n",
        "            im_info = {\n",
        "                'img_id': img_id,\n",
        "                'filename': img_path,\n",
        "                'width': width,\n",
        "                'height': height,\n",
        "                'detections': detections\n",
        "            }\n",
        "            im_infos.append(im_info)\n",
        "\n",
        "    return im_infos\n",
        "\n",
        "def collate_function(data):\n",
        "    return tuple(zip(*data))"
      ],
      "metadata": {
        "id": "Tc_W4Opoond2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = '/content/drive/MyDrive/Colab Notebooks/LLVIP DATASET/LLVIP_IMG/test'\n",
        "\n",
        "annotation_dir = '/content/drive/MyDrive/Colab Notebooks/LLVIP DATASET/LLVIP_LABEL/Annotations'"
      ],
      "metadata": {
        "id": "pat_3NkWpmUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label2idx = {'person': 1, 'car': 2}  # Adjust based on your model's labels\n",
        "classes = ['background', 'person', 'car']  # Add all classes, including background\n",
        "\n",
        "image_info = LLVIP_get_image_info(image_dir, annotation_dir, label2idx, classes)\n",
        "\n",
        "print(len(image_info))\n",
        "print(image_info[0] if image_info else \"No valid images found\")\n",
        "\n",
        "# Example output\n",
        "for info in image_info[:2]:  # Print the first 2 entries\n",
        "    print(info)"
      ],
      "metadata": {
        "id": "2B6t4TOOpnOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = LLVIPDataset(im_dir=image_dir, ann_dir=annotation_dir)"
      ],
      "metadata": {
        "id": "AOxuCOzDmoBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e2bd19-92c9-4eea-b4c8-9965c32a6901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3463 images.\n",
            "First entry: {'img_id': '190006', 'filename': '/content/drive/MyDrive/Colab Notebooks/LLVIP DATASET/LLVIP_IMG/test/190006.jpg', 'width': 1280, 'height': 1024, 'detections': [{'label': 1, 'bbox': [114, 382, 212, 679]}, {'label': 1, 'bbox': [31, 407, 122, 665]}, {'label': 1, 'bbox': [20, 237, 87, 459]}, {'label': 1, 'bbox': [75, 212, 174, 459]}, {'label': 1, 'bbox': [1219, 332, 1279, 559]}, {'label': 1, 'bbox': [1, 284, 26, 528]}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_ratios = [0.7, 0.15, 0.15]\n",
        "train_dataset, val_dataset, test_dataset = split_dataset(dataset, split_ratios)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=collate_function)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=collate_function)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=collate_function)"
      ],
      "metadata": {
        "id": "7LwdRXYRmrT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d2a1a32-aff9-4121-886a-dd8b46acf5f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' Hyperparameters\n",
        "    Edit the values as necessary here '''\n",
        "\n",
        "rpn_pre_nms_top_n = 2000            # Number of proposals before NMS (filtering proposals)\n",
        "rpn_post_nms_top_n = 1000           # Number of proposals before NMS (filtering proposals)\n",
        "rpn_nms_thresh = 0.7                # IoU threshold for NMS\n",
        "rpn_fg_iou_thresh = 0.7             # Minimum IoU for a positive anchor\n",
        "rpn_bg_iou_thresh = 0.3             # Maximum IoU for a negative anchor\n",
        "rpn_batch_size_per_image = 256      # Number of anchors sampled per image during training\n",
        "\n",
        "box_detections_per_img = 9          # Maximum number of detections per image\n",
        "box_score_thresh = 0.05             # Min. score for a detection to be considered valid\n",
        "box_nms_thresh = 0.5                # IoU threshold for NMS on detection results\n",
        "box_fg_iou_thresh = 0.5             # Minimum IoU for a positive RoI during training\n",
        "box_bg_iou_thresh = 0.5             # Maximum IoU for a negative RoI during training\n",
        "\n",
        "batch_size = 4\n",
        "lr = 0.003\n",
        "num_epochs = 100\n",
        "momentum = 0.9\n",
        "weight_decay = 0.005\n",
        "acc_steps = 1                       # Can leave as 1\n",
        "\n",
        "loss_weights = [1, 1, 1, 1]"
      ],
      "metadata": {
        "id": "KmD9Hpx0m1nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True,\n",
        "                                                             min_size=512,\n",
        "                                                             max_size=640,\n",
        "                                                             rpn_pre_nms_top_n=rpn_pre_nms_top_n,\n",
        "                                                             rpn_post_nms_top_n=rpn_post_nms_top_n,\n",
        "                                                             rpn_nms_thresh=rpn_nms_thresh,\n",
        "                                                             rpn_fg_iou_thresh=rpn_fg_iou_thresh,\n",
        "                                                             rpn_bg_iou_thresh=rpn_bg_iou_thresh,\n",
        "                                                             rpn_batch_size_per_image=rpn_batch_size_per_image,\n",
        "                                                             box_detections_per_img=box_detections_per_img,\n",
        "                                                             box_nms_thresh=box_nms_thresh,\n",
        "                                                             box_score_thresh=box_score_thresh,\n",
        "                                                             box_fg_iou_thresh=box_fg_iou_thresh,\n",
        "                                                             box_bg_iou_thresh=box_bg_iou_thresh,\n",
        ")\n",
        "\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(\n",
        "            model.roi_heads.box_predictor.cls_score.in_features,\n",
        "            num_classes=3)\n",
        "\n",
        "optimizer = torch.optim.SGD(lr=lr, momentum=momentum, weight_decay=weight_decay,\n",
        "                            params=filter(lambda p: p.requires_grad, model.parameters()))"
      ],
      "metadata": {
        "id": "u1ejT3-jnCGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, dataset, train_loader, val_loader, optimizer, loss_weights, num_epochs, acc_steps, device)"
      ],
      "metadata": {
        "id": "J4BirUvNnIEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/content/model_frcnn_lr0.003_epoch99', weights_only=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt8tXx2oAFLt",
        "outputId": "70ffa43a-a6cd-4809-ec5b-5f3c87a5dccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_map(model, dataset, test_loader, device)"
      ],
      "metadata": {
        "id": "mn3Hh5WgAQQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infer(model, dataset, test_loader, 'yes', device, 0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REdbRWMFBHSx",
        "outputId": "4de4cbf5-4530-44f1-f639-f2753742524e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:01<00:00,  5.20it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters to improve recall\n",
        "rpn_pre_nms_top_n = 3000            # Number of proposals before NMS (filtering proposals)\n",
        "rpn_post_nms_top_n = 1500           # Number of proposals before NMS (filtering proposals)\n",
        "rpn_nms_thresh = 0.6               # IoU threshold for NMS\n",
        "rpn_fg_iou_thresh = 0.6             # Minimum IoU for a positive anchor\n",
        "rpn_bg_iou_thresh = 0.4             # Maximum IoU for a negative anchor\n",
        "rpn_batch_size_per_image = 512      # Number of anchors sampled per image during training\n",
        "\n",
        "box_detections_per_img = 9          # Maximum number of detections per image\n",
        "box_score_thresh = 0.01             # Min. score for a detection to be considered valid\n",
        "box_nms_thresh = 0.4                # IoU threshold for NMS on detection results\n",
        "box_fg_iou_thresh = 0.4             # Minimum IoU for a positive RoI during training\n",
        "box_bg_iou_thresh = 0.5             # Maximum IoU for a negative RoI during training"
      ],
      "metadata": {
        "id": "fdgGiwpdONgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ADA FLIR DATASET**"
      ],
      "metadata": {
        "id": "X3IOuqQwDxl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "\n",
        "class FLIRDataset(Dataset):\n",
        "    def __init__(self, im_dir, ann_dir):\n",
        "        self.im_dir = im_dir\n",
        "        self.ann_dir = ann_dir\n",
        "\n",
        "        # Define the classes\n",
        "        self.classes = ['person', 'vehicle']\n",
        "        self.classes = sorted(self.classes)\n",
        "        self.classes = ['background'] + self.classes  # Background is index 0\n",
        "\n",
        "        self.label2idx = {self.classes[idx]: idx for idx in range(len(self.classes))}\n",
        "        self.idx2label = {idx: self.classes[idx] for idx in range(len(self.classes))}\n",
        "\n",
        "        # Load image information\n",
        "        #self.label2idx = {1:1, 3:3}\n",
        "        #self.idx2label = {1:1, 3:3}\n",
        "        #self.classes = [0, 1, 3]\n",
        "\n",
        "        # Keep the label2idx and classes as is, create new variables for the image information\n",
        "        self.images_info = FLIR_get_image_info(im_dir, ann_dir, {1:1, 3:2}, [0, 1, 2])\n",
        "\n",
        "        print(f\"Loaded {len(self.images_info)} images.\")\n",
        "        if self.images_info:\n",
        "            print(f\"First entry: {self.images_info[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_info)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        im_info = self.images_info[index]\n",
        "        im = Image.open(im_info['filename']).convert('RGB')\n",
        "\n",
        "        # Convert image to tensor\n",
        "        im_tensor = torchvision.transforms.ToTensor()(im)\n",
        "\n",
        "        # Prepare target (bounding boxes and labels)\n",
        "        targets = {}\n",
        "        targets['bboxes'] = torch.as_tensor([detection['bbox'] for detection in im_info['detections']])\n",
        "        targets['labels'] = torch.as_tensor([detection['label'] for detection in im_info['detections']])\n",
        "\n",
        "        return im_tensor, targets, im_info['filename']\n",
        "\n",
        "\n",
        "\n",
        "def collate_function(data):\n",
        "    return tuple(zip(*data))"
      ],
      "metadata": {
        "id": "jLDg5lV3227G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FLIR_get_image_info(image_dir, annotation_file, label2idx, classes):\n",
        "    import os\n",
        "    import cv2\n",
        "    import json\n",
        "\n",
        "    # Read the JSON annotation file\n",
        "    with open(annotation_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    im_infos = []\n",
        "\n",
        "    # Create a mapping of image IDs to file paths\n",
        "    image_id_to_filename = {\n",
        "        img['id']: os.path.join(image_dir, os.path.basename(img['file_name']))\n",
        "        for img in data['images']\n",
        "    }\n",
        "\n",
        "    # Parse annotations and group them by image ID\n",
        "    annotations_by_image = {}\n",
        "    for ann in data['annotations']:\n",
        "        img_id = ann['image_id']\n",
        "        if img_id not in annotations_by_image:\n",
        "            annotations_by_image[img_id] = []\n",
        "        annotations_by_image[img_id].append(ann)\n",
        "\n",
        "    # Process each image\n",
        "    for img_id, detections in annotations_by_image.items():\n",
        "        if img_id not in image_id_to_filename:\n",
        "            continue\n",
        "\n",
        "        filename = image_id_to_filename[img_id]\n",
        "        if not os.path.exists(filename):\n",
        "            print(f\"Image file {filename} does not exist.\")\n",
        "            continue\n",
        "\n",
        "        # Read the image dimensions\n",
        "        image = cv2.imread(filename)\n",
        "        if image is None:\n",
        "            print(f\"Failed to read image {filename}.\")\n",
        "            continue\n",
        "        height, width, _ = image.shape\n",
        "\n",
        "        valid_detections = []\n",
        "        for det in detections:\n",
        "            category_id = det['category_id']\n",
        "\n",
        "            # Handle only known categories: {1: person, 3: car}\n",
        "            if category_id == 1:\n",
        "                label = 1  # Person\n",
        "            elif category_id == 3:\n",
        "                label = 2  # Remap Car (category_id=3) to label=2\n",
        "            else:\n",
        "                #print(f\"Skipping unknown category_id={category_id}\")\n",
        "                continue\n",
        "\n",
        "            if label not in classes:\n",
        "                print(f\"Skipping detection with category_id={category_id} because label {label} is not in classes.\")\n",
        "                continue\n",
        "\n",
        "            # Bounding box is [x_min, y_min, width, height]\n",
        "            bbox = det['bbox']\n",
        "            x_min, y_min, w, h = bbox\n",
        "            x_max, y_max = x_min + w, y_min + h\n",
        "            valid_detections.append({\n",
        "                'label': label,\n",
        "                'bbox': [x_min, y_min, x_max, y_max]\n",
        "            })\n",
        "\n",
        "        if not valid_detections:\n",
        "            print(f\"No valid detections for image ID {img_id}.\")\n",
        "            continue\n",
        "\n",
        "        im_infos.append({\n",
        "            'img_id': img_id,\n",
        "            'filename': filename,\n",
        "            'width': width,\n",
        "            'height': height,\n",
        "            'detections': valid_detections\n",
        "        })\n",
        "\n",
        "    print(f\"Total processed images: {len(im_infos)}\")\n",
        "    return im_infos"
      ],
      "metadata": {
        "id": "wrFjk2RWBeIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = '/content/drive/MyDrive/Colab Notebooks/ADA FLIR DATASET/thermal_8_bit'\n",
        "\n",
        "annotation_dir = '/content/drive/MyDrive/Colab Notebooks/ADA FLIR DATASET/thermal_annotations.json'"
      ],
      "metadata": {
        "id": "so5yPKeeok1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = FLIRDataset(im_dir=image_dir, ann_dir=annotation_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ7q8qt0onqE",
        "outputId": "caf5266d-c530-4909-c1c1-bb58ced2a43d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total processed images: 1360\n",
            "Loaded 1360 images.\n",
            "First entry: {'img_id': 0, 'filename': '/content/drive/MyDrive/Colab Notebooks/ADA FLIR DATASET/thermal_8_bit/FLIR_08863.jpeg', 'width': 640, 'height': 512, 'detections': [{'label': 1, 'bbox': [580, 206, 614, 281]}, {'label': 1, 'bbox': [609, 207, 639, 323]}, {'label': 2, 'bbox': [469, 226, 499, 252]}, {'label': 2, 'bbox': [462, 230, 477, 244]}, {'label': 2, 'bbox': [293, 227, 332, 253]}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rpn_pre_nms_top_n = 2000            # Number of proposals before NMS (filtering proposals)\n",
        "rpn_post_nms_top_n = 1000           # Number of proposals after NMS (filtering proposals)\n",
        "rpn_nms_thresh = 0.7                # IoU threshold for NMS\n",
        "rpn_fg_iou_thresh = 0.7             # Minimum IoU for a positive anchor\n",
        "rpn_bg_iou_thresh = 0.3             # Maximum IoU for a negative anchor\n",
        "rpn_batch_size_per_image = 256      # Number of anchors sampled per image during training\n",
        "\n",
        "box_detections_per_img = 9          # Maximum number of detections per image\n",
        "box_score_thresh = 0.05             # Min. score for a detection to be considered valid\n",
        "box_nms_thresh = 0.5                # IoU threshold for NMS on detection results\n",
        "box_fg_iou_thresh = 0.5             # Minimum IoU for a positive RoI during training\n",
        "box_bg_iou_thresh = 0.5             # Maximum IoU for a negative RoI during training\n",
        "\n",
        "batch_size = 4\n",
        "lr = 0.003\n",
        "num_epochs = 100\n",
        "momentum = 0.9\n",
        "weight_decay = 0.005\n",
        "acc_steps = 1                       # Can leave as 1\n",
        "\n",
        "loss_weights = [1, 1, 1, 1]"
      ],
      "metadata": {
        "id": "HbDm0gnyoy0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True,\n",
        "                                                             min_size=512,\n",
        "                                                             max_size=640,\n",
        "                                                             rpn_pre_nms_top_n=rpn_pre_nms_top_n,\n",
        "                                                             rpn_post_nms_top_n=rpn_post_nms_top_n,\n",
        "                                                             rpn_nms_thresh=rpn_nms_thresh,\n",
        "                                                             rpn_fg_iou_thresh=rpn_fg_iou_thresh,\n",
        "                                                             rpn_bg_iou_thresh=rpn_bg_iou_thresh,\n",
        "                                                             rpn_batch_size_per_image=rpn_batch_size_per_image,\n",
        "                                                             box_detections_per_img=box_detections_per_img,\n",
        "                                                             box_nms_thresh=box_nms_thresh,\n",
        "                                                             box_score_thresh=box_score_thresh,\n",
        "                                                             box_fg_iou_thresh=box_fg_iou_thresh,\n",
        "                                                             box_bg_iou_thresh=box_bg_iou_thresh,\n",
        ")\n",
        "\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(\n",
        "            model.roi_heads.box_predictor.cls_score.in_features,\n",
        "            num_classes=3)\n",
        "\n",
        "optimizer = torch.optim.SGD(lr=lr, momentum=momentum, weight_decay=weight_decay,\n",
        "                            params=filter(lambda p: p.requires_grad, model.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pgj5MYy_4V00",
        "outputId": "1895ea45-bc16-438a-91b5-18df18af4316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_ratios = [0.7, 0.15, 0.15]\n",
        "train_dataset, val_dataset, test_dataset = split_dataset(dataset, split_ratios)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=collate_function)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=collate_function)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=collate_function)"
      ],
      "metadata": {
        "id": "QzpQre8M4Ze2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/content/model_frcnn_lr0.003_epoch149', map_location='cpu'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7IYBsnto2pC",
        "outputId": "eb322ebe-85b9-43ea-8ee8-7fbb098a92b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-7195dc34106a>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/model_frcnn_lr0.003_epoch149', map_location='cpu'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_map(model, dataset, test_loader, device)"
      ],
      "metadata": {
        "id": "79_HLbf3o01N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pascal_map(model, dataset, test_loader, device)"
      ],
      "metadata": {
        "id": "cCeutS0_EORX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"COCO_V1\")"
      ],
      "metadata": {
        "id": "Z2e282a2CCIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infer(model, dataset, test_loader, 'sample', device, 0.5)"
      ],
      "metadata": {
        "id": "dhyJWKBYGWB_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}