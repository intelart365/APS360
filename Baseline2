import os
import zipfile
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision import models
from torch.utils.data import DataLoader, Dataset
from PIL import Image, ImageDraw
import matplotlib.pyplot as plt
from google.colab import files

uploaded = files.upload()  

with zipfile.ZipFile("my_dataset.zip", "r") as zip_ref:
    zip_ref.extractall("my_dataset")

class CustomDataset(Dataset):
    def __init__(self, images_dir, labels_dir, transform=None, max_objects=8):
        self.images_dir = images_dir
        self.labels_dir = labels_dir
        self.transform = transform
        self.max_objects = max_objects
        self.image_filenames = os.listdir(images_dir)

    def __len__(self):
        return len(self.image_filenames)

    def __getitem__(self, idx):
        image_filename = self.image_filenames[idx]
        label_filename = image_filename.replace(".jpg", ".txt")
        
        image_path = os.path.join(self.images_dir, image_filename)
        image = Image.open(image_path).convert("RGB") 
        
        if self.transform:
            image = self.transform(image)
        
        label_path = os.path.join(self.labels_dir, label_filename)
        with open(label_path, "r") as file:
            labels = file.readlines()
        
        bboxes = []
        for label in labels:
            parts = label.strip().split()
            class_id = int(parts[0])
            x_center = float(parts[1])
            y_center = float(parts[2])
            width = float(parts[3])
            height = float(parts[4])
            bboxes.append([class_id, x_center, y_center, width, height])
        
        while len(bboxes) < self.max_objects:
            bboxes.append([0, 0.0, 0.0, 0.0, 0.0])  
        
        return image, torch.tensor(bboxes[:self.max_objects]) 

class ObjectDetectionCNN(nn.Module):
    def __init__(self, num_classes=2, max_objects=8):
        super(ObjectDetectionCNN, self).__init__()
        
        self.feature_extractor = models.resnet18(pretrained=True)
        self.feature_extractor.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False) 
        
        in_features = self.feature_extractor.fc.in_features
        self.feature_extractor.fc = nn.Identity() 
        
        self.fc_class = nn.Linear(in_features, num_classes * max_objects)
        self.fc_bbox = nn.Linear(in_features, 4 * max_objects)

        self.num_classes = num_classes
        self.max_objects = max_objects

    def forward(self, x):
        x = self.feature_extractor(x)
        class_logits = self.fc_class(x).view(-1, self.max_objects, self.num_classes)
        bbox_preds = self.fc_bbox(x).view(-1, self.max_objects, 4)
        return class_logits, bbox_preds

transform = transforms.Compose([
    transforms.Resize((512, 640)),
    transforms.RandomHorizontalFlip(), 
    transforms.ToTensor()
])

dataset = CustomDataset(images_dir='my_dataset/images', labels_dir='my_dataset/labels', transform=transform)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

model = ObjectDetectionCNN(num_classes=2, max_objects=8)
model.train()

criterion_class = nn.CrossEntropyLoss()
criterion_bbox = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)  

train_losses = []

for epoch in range(50):  
    epoch_loss = 0
    for images, targets in dataloader:
        optimizer.zero_grad()
        
        class_logits, bbox_preds = model(images)
        
        class_targets = targets[..., 0].long()  
        bbox_targets = targets[..., 1:]        
        
        loss_class = criterion_class(class_logits.view(-1, 2), class_targets.view(-1))
        loss_bbox = criterion_bbox(bbox_preds, bbox_targets)
        
        loss = loss_class + loss_bbox
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
    
    avg_epoch_loss = epoch_loss / len(dataloader)
    train_losses.append(avg_epoch_loss)
    print(f"Epoch {epoch+1}, Loss: {avg_epoch_loss:.4f}")

print("Training Done")

plt.plot(train_losses, label='Train Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

def draw_bounding_boxes(image, class_logits, bbox_preds, threshold=0.3):  
    draw = ImageDraw.Draw(image)
    for i in range(len(class_logits)):
        class_id = torch.argmax(class_logits[i]).item()
        confidence = torch.max(class_logits[i]).item()
        if confidence < threshold:
            continue 
        
        x_center, y_center, width, height = bbox_preds[i].tolist()

        x1 = (x_center - width / 2) * image.width
        y1 = (y_center - height / 2) * image.height
        x2 = (x_center + width / 2) * image.width
        y2 = (y_center + height / 2) * image.height
        
        print(f"Drawing box: Class {class_id}, Confidence {confidence}, Box: ({x1}, {y1}, {x2}, {y2})")

        if x1 < 0 or y1 < 0 or x2 < 0 or y2 < 0 or y1 > y2 or x1 > x2:
            continue  

        label = "Person" if class_id == 0 else "Car"
        color = "red" if class_id == 0 else "blue"
        
        draw.rectangle([x1, y1, x2, y2], outline=color, width=2)
        draw.text((x1, y1), label, fill=color)

test_image_path = 'my_dataset/images/video_frame_003182.jpg'
test_image = Image.open(test_image_path).convert("RGB")
test_image_tensor = transform(test_image).unsqueeze(0)

model.eval()
with torch.no_grad():
    class_logits, bbox_preds = model(test_image_tensor)
    class_logits, bbox_preds = class_logits[0], bbox_preds[0]
    
draw_bounding_boxes(test_image, class_logits, bbox_preds)

plt.imshow(test_image)
plt.axis('off')
plt.show()
